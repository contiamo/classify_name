{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Classifying Names with a Character-Level RNN\n",
    "*********************************************\n",
    "We will be building and training a basic character-level RNN to classify\n",
    "words. A character-level RNN reads words as a series of characters -\n",
    "outputting a prediction and \"hidden state\" at each step, feeding its\n",
    "previous hidden state into each next step. We take the final prediction\n",
    "to be the output, i.e. which class the word belongs to.\n",
    "\n",
    "Specifically, we'll train on a few thousand surnames from 18 languages\n",
    "of origin, and predict which language a name is from based on the\n",
    "spelling:\n",
    "\n",
    "::\n",
    "\n",
    "    $ python predict.py Hinton\n",
    "    (-0.47) Scottish\n",
    "    (-1.52) English\n",
    "    (-3.57) Irish\n",
    "\n",
    "    $ python predict.py Schmidhuber\n",
    "    (-0.19) German\n",
    "    (-2.48) Czech\n",
    "    (-2.68) Dutch\n",
    "\n",
    "\n",
    "**Recommended Reading:**\n",
    "\n",
    "I assume you have at least installed PyTorch, know Python, and\n",
    "understand Tensors:\n",
    "\n",
    "-  http://pytorch.org/ For installation instructions\n",
    "-  :doc:`/beginner/deep_learning_60min_blitz` to get started with PyTorch in general\n",
    "-  :doc:`/beginner/pytorch_with_examples` for a wide and deep overview\n",
    "-  :doc:`/beginner/former_torchies_tutorial` if you are former Lua Torch user\n",
    "\n",
    "It would also be useful to know about RNNs and how they work:\n",
    "\n",
    "-  `The Unreasonable Effectiveness of Recurrent Neural\n",
    "   Networks <http://karpathy.github.io/2015/05/21/rnn-effectiveness/>`__\n",
    "   shows a bunch of real life examples\n",
    "-  `Understanding LSTM\n",
    "   Networks <http://colah.github.io/posts/2015-08-Understanding-LSTMs/>`__\n",
    "   is about LSTMs specifically but also informative about RNNs in\n",
    "   general\n",
    "\n",
    "Preparing the Data\n",
    "==================\n",
    "\n",
    ".. Note::\n",
    "   Download the data from\n",
    "   `here <https://download.pytorch.org/tutorial/data.zip>`_\n",
    "   and extract it to the current directory.\n",
    "\n",
    "Included in the ``data/names`` directory are 18 text files named as\n",
    "\"[Language].txt\". Each file contains a bunch of names, one name per\n",
    "line, mostly romanized (but we still need to convert from Unicode to\n",
    "ASCII).\n",
    "\n",
    "We'll end up with a dictionary of lists of names per language,\n",
    "``{language: [names ...]}``. The generic variables \"category\" and \"line\"\n",
    "(for language and name in our case) are used for later extensibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import importlib\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "import torch\n",
    "import unicodedata\n",
    "\n",
    "nn = torch.nn\n",
    "import matplotlib as mpl\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "from pathlib import PurePath, Path\n",
    "\n",
    "bundle_root = Path(os.environ['LABS_BUNDLE_ROOT'])\n",
    "sys.path.append(str(PurePath(bundle_root, 'functions')))\n",
    "sys.path.append(str(PurePath(bundle_root, 'common')))\n",
    "import utils\n",
    "importlib.reload(utils);\n",
    "COLOR = 'white'\n",
    "mpl.rcParams['text.color'] = COLOR\n",
    "mpl.rcParams['axes.labelcolor'] = COLOR\n",
    "mpl.rcParams['xtick.color'] = COLOR\n",
    "mpl.rcParams['ytick.color'] = COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slusarski\n",
      "German\n",
      "English\n",
      "Czech\n",
      "Portuguese\n",
      "Japanese\n",
      "Polish\n",
      "Chinese\n",
      "Scottish\n",
      "Spanish\n",
      "Irish\n",
      "French\n",
      "Italian\n",
      "Russian\n",
      "Vietnamese\n",
      "Greek\n",
      "Arabic\n",
      "Dutch\n",
      "Korean\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for path in (bundle_root / 'data/raw/names').glob('*.txt'):\n",
    "    path = str(path)\n",
    "    category = path.split('/')[-1].split('.')[0]\n",
    "    print(category)\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(path)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning Names into Tensors\n",
    "--------------------------\n",
    "\n",
    "Now that we have all the names organized, we need to turn them into\n",
    "Tensors to make any use of them.\n",
    "\n",
    "To represent a single letter, we use a \"one-hot vector\" of size\n",
    "``<1 x n_letters>``. A one-hot vector is filled with 0s except for a 1\n",
    "at index of the current letter, e.g. ``\"b\" = <0 1 0 0 0 ...>``.\n",
    "\n",
    "To make a word we join a bunch of those into a 2D matrix\n",
    "``<line_length x 1 x n_letters>``.\n",
    "\n",
    "That extra 1 dimension is because PyTorch assumes everything is in\n",
    "batches - we're just using a batch size of 1 here.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]])\n",
      "torch.Size([5, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letterToTensor('J'))\n",
    "\n",
    "print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Network\n",
    "====================\n",
    "\n",
    "Before autograd, creating a recurrent neural network in Torch involved\n",
    "cloning the parameters of a layer over several timesteps. The layers\n",
    "held hidden state and gradients which are now entirely handled by the\n",
    "graph itself. This means you can implement a RNN in a very \"pure\" way,\n",
    "as regular feed-forward layers.\n",
    "\n",
    "This RNN module (mostly copied from `the PyTorch for Torch users\n",
    "tutorial <http://pytorch.org/tutorials/beginner/former_torchies/\n",
    "nn_tutorial.html#example-2-recurrent-net>`__)\n",
    "is just 2 linear layers which operate on an input and hidden state, with\n",
    "a LogSoftmax layer after the output.\n",
    "\n",
    ".. figure:: https://i.imgur.com/Z2xbySO.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "rnn = utils.RNN(utils.n_letters, utils.n_hidden, utils.n_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a step of this network we need to pass an input (in our case, the\n",
    "Tensor for the current letter) and a previous hidden state (which we\n",
    "initialize as zeros at first). We'll get back the output (probability of\n",
    "each language) and a next hidden state (which we keep for the next\n",
    "step).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = letterToTensor('A')\n",
    "hidden =torch.zeros(1, utils.n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of efficiency we don't want to be creating a new Tensor for\n",
    "every step, so we will use ``lineToTensor`` instead of\n",
    "``letterToTensor`` and use slices. This could be further optimized by\n",
    "pre-computing batches of Tensors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.7946, -2.8446, -2.8903, -2.8633, -2.9527, -2.9048, -2.8922, -2.8723,\n",
      "         -2.9992, -2.8841, -2.8545, -2.8749, -2.8988, -2.8751, -2.8291, -3.0311,\n",
      "         -2.9775, -2.8202]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = lineToTensor('Albert')\n",
    "hidden = torch.zeros(1, utils.n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the output is a ``<1 x n_categories>`` Tensor, where\n",
    "every item is the likelihood of that category (higher is more likely).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "========\n",
    "Preparing for Training\n",
    "----------------------\n",
    "\n",
    "Before going into training we should make a few helper functions. The\n",
    "first is to interpret the output of the network, which we know to be a\n",
    "likelihood of each category. We can use ``Tensor.topk`` to get the index\n",
    "of the greatest value:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('German', 0)\n"
     ]
    }
   ],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also want a quick way to get a training example (a name and its\n",
    "language):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = Korean / line = Kwang \n",
      "category = Chinese / line = Yao\n",
      "category = Arabic / line = Nader\n",
      "category = English / line = Neighbour\n",
      "category = Arabic / line = Gerges\n",
      "category = Russian / line = Chuhman\n",
      "category = Vietnamese / line = Dang\n",
      "category = Scottish / line = Watson\n",
      "category = Czech / line = Zak\n",
      "category = Greek / line = Vassilikos\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Network\n",
    "--------------------\n",
    "\n",
    "Now all it takes to train this network is show it a bunch of examples,\n",
    "have it make guesses, and tell it if it's wrong.\n",
    "\n",
    "For the loss function ``nn.NLLLoss`` is appropriate, since the last\n",
    "layer of the RNN is ``nn.LogSoftmax``.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each loop of training will:\n",
    "\n",
    "-  Create input and target tensors\n",
    "-  Create a zeroed initial hidden state\n",
    "-  Read each letter in and\n",
    "\n",
    "   -  Keep hidden state for next letter\n",
    "\n",
    "-  Compare final output to target\n",
    "-  Back-propagate\n",
    "-  Return the output and loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to run that with a bunch of examples. Since the\n",
    "``train`` function returns both the output and loss we can print its\n",
    "guesses and also keep track of loss for plotting. Since there are 1000s\n",
    "of examples we print only every ``print_every`` examples, and take an\n",
    "average of the loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0% (0m 0s) 2.9145 Amari / Italian ✗ (Arabic)\n",
      "1000 0% (0m 1s) 2.8645 Adleroff / Scottish ✗ (Russian)\n",
      "1500 0% (0m 2s) 2.7455 Brzezicki / Italian ✗ (Polish)\n",
      "2000 0% (0m 3s) 2.7198 Araullo / Portuguese ✓\n",
      "2500 0% (0m 4s) 2.6093 Holzer / German ✓\n",
      "3000 0% (0m 6s) 2.7179 Weichert / Dutch ✗ (Czech)\n",
      "3500 0% (0m 7s) 2.2370 Jolkov / Russian ✓\n",
      "4000 0% (0m 8s) 2.7892 Derrick / Russian ✗ (German)\n",
      "4500 0% (0m 9s) 3.0385 Perrault / Dutch ✗ (French)\n",
      "5000 0% (0m 11s) 2.3343 Lauwers / Greek ✗ (Dutch)\n",
      "5500 0% (0m 12s) 2.5870 Taylor / Arabic ✗ (Scottish)\n",
      "6000 0% (0m 13s) 2.5263 Hildebrand / French ✗ (German)\n",
      "6500 0% (0m 13s) 1.6865 Tsang / Chinese ✓\n",
      "7000 0% (0m 14s) 2.6278 Perrin / Scottish ✗ (English)\n",
      "7500 0% (0m 15s) 1.5070 Thai / Chinese ✗ (Vietnamese)\n",
      "8000 0% (0m 15s) 3.0970 Zozulya / Japanese ✗ (Russian)\n",
      "8500 0% (0m 16s) 1.6039 Ha / Vietnamese ✓\n",
      "9000 0% (0m 17s) 2.0638 Taylor / Scottish ✓\n",
      "9500 0% (0m 19s) 1.3929 Yin / Korean ✗ (Chinese)\n",
      "10000 1% (0m 21s) 2.2070 Araujo / Arabic ✗ (Portuguese)\n",
      "10500 1% (0m 22s) 2.4244 Harb / Chinese ✗ (Arabic)\n",
      "11000 1% (0m 23s) 2.4185 Fabron / Irish ✗ (French)\n",
      "11500 1% (0m 24s) 2.1666 Romero / Portuguese ✗ (Spanish)\n",
      "12000 1% (0m 26s) 1.8214 Araullo / Italian ✗ (Portuguese)\n",
      "12500 1% (0m 27s) 3.0243 Spada / Japanese ✗ (Italian)\n",
      "13000 1% (0m 29s) 1.3510 Asghar / Arabic ✓\n",
      "13500 1% (0m 31s) 2.0163 Luong / Chinese ✗ (Vietnamese)\n",
      "14000 1% (0m 33s) 2.4009 Niemec / Dutch ✗ (Polish)\n",
      "14500 1% (0m 36s) 1.3582 Kanaan / Arabic ✓\n",
      "15000 1% (0m 38s) 2.4080 Araya / Japanese ✗ (Spanish)\n",
      "15500 1% (0m 39s) 2.2040 Dufort / Arabic ✗ (French)\n",
      "16000 1% (0m 40s) 1.3797 Inihara / Japanese ✓\n",
      "16500 1% (0m 42s) 2.1510 Abreu / Spanish ✗ (Portuguese)\n",
      "17000 1% (0m 44s) 1.4336 Wei / Vietnamese ✗ (Chinese)\n",
      "17500 1% (0m 45s) 2.3581 Mulligan / Irish ✗ (English)\n",
      "18000 1% (0m 46s) 1.0353 Wojewodka / Czech ✗ (Polish)\n",
      "18500 1% (0m 47s) 1.4676 Rim / Korean ✓\n",
      "19000 1% (0m 48s) 1.8383 Dakhin / Japanese ✗ (Russian)\n",
      "19500 1% (0m 48s) 2.3016 Singh / German ✗ (English)\n",
      "20000 2% (0m 49s) 2.3356 Machado / Japanese ✗ (Portuguese)\n",
      "20500 2% (0m 50s) 1.0419 Hanabusa / Japanese ✓\n",
      "21000 2% (0m 51s) 0.6458 Kagawa / Japanese ✓\n",
      "21500 2% (0m 52s) 2.3603 Cham / Vietnamese ✗ (Arabic)\n",
      "22000 2% (0m 53s) 1.7635 Fournier / German ✗ (French)\n",
      "22500 2% (0m 54s) 1.2368 Napoliello / Spanish ✗ (Italian)\n",
      "23000 2% (0m 55s) 0.9276 Hew / Chinese ✓\n",
      "23500 2% (0m 56s) 0.8050 Luu / Vietnamese ✓\n",
      "24000 2% (0m 57s) 1.3869 Laren / Dutch ✓\n",
      "24500 2% (0m 58s) 2.0038 Mcgrady / Scottish ✗ (English)\n",
      "25000 2% (0m 59s) 1.0889 Seo / Chinese ✗ (Korean)\n",
      "25500 2% (1m 0s) 0.4306 Rooijakkers / Dutch ✓\n",
      "26000 2% (1m 2s) 1.3480 Morrison / Scottish ✓\n",
      "26500 2% (1m 3s) 1.2587 Leikin / Russian ✓\n",
      "27000 2% (1m 5s) 4.9658 Shakhgildyan / Japanese ✗ (Russian)\n",
      "27500 2% (1m 6s) 1.1418 De santigo / Portuguese ✓\n",
      "28000 2% (1m 6s) 1.1360 Macleod / Scottish ✓\n",
      "28500 2% (1m 7s) 2.9182 Rompaij / Russian ✗ (Dutch)\n",
      "29000 2% (1m 8s) 1.3478 Hou / Korean ✗ (Chinese)\n",
      "29500 2% (1m 9s) 1.2400 Ogura / Japanese ✓\n",
      "30000 3% (1m 10s) 1.4615 Kerwar / German ✓\n",
      "30500 3% (1m 11s) 2.6349 Pinter / German ✗ (Czech)\n",
      "31000 3% (1m 12s) 1.3841 Abyzgiddin / Polish ✗ (Russian)\n",
      "31500 3% (1m 12s) 1.6175 Wood / Scottish ✓\n",
      "32000 3% (1m 13s) 1.4803 Doherty / Irish ✓\n",
      "32500 3% (1m 14s) 2.1835 Fritsch / Scottish ✗ (Czech)\n",
      "33000 3% (1m 15s) 1.6645 Arthur / German ✗ (French)\n",
      "33500 3% (1m 16s) 2.1543 Crawford / French ✗ (Scottish)\n",
      "34000 3% (1m 17s) 1.8582 Mitsui / Arabic ✗ (Japanese)\n",
      "34500 3% (1m 18s) 4.3300 Maly / Irish ✗ (Polish)\n",
      "35000 3% (1m 19s) 1.7875 Barros / Portuguese ✗ (Spanish)\n",
      "35500 3% (1m 20s) 2.2559 Noyce / Irish ✗ (English)\n",
      "36000 3% (1m 22s) 2.0077 Del olmo / Italian ✗ (Spanish)\n",
      "36500 3% (1m 23s) 1.4321 D'cruz / Spanish ✗ (Portuguese)\n",
      "37000 3% (1m 25s) 0.5815 Dam / Vietnamese ✓\n",
      "37500 3% (1m 25s) 2.3883 Bach / German ✗ (Vietnamese)\n",
      "38000 3% (1m 26s) 0.9683 Bui / Chinese ✓\n",
      "38500 3% (1m 27s) 1.4302 Vilaro / Spanish ✓\n",
      "39000 3% (1m 27s) 1.7990 Hartmann / Scottish ✗ (German)\n",
      "39500 3% (1m 28s) 0.7596 O'Neill / Irish ✓\n",
      "40000 4% (1m 29s) 0.9121 Ha / Korean ✓\n",
      "40500 4% (1m 29s) 1.4777 Bohmer / German ✓\n",
      "41000 4% (1m 30s) 1.9899 Adlersflugel / Dutch ✗ (German)\n",
      "41500 4% (1m 31s) 0.6496 Avilov / Russian ✓\n",
      "42000 4% (1m 31s) 1.6353 Paternoster / German ✗ (French)\n",
      "42500 4% (1m 32s) 2.9427 Chastain / Irish ✗ (French)\n",
      "43000 4% (1m 33s) 0.3240 Pietri / Italian ✓\n",
      "43500 4% (1m 33s) 1.4802 Walentowicz / Polish ✓\n",
      "44000 4% (1m 34s) 1.2731 Kucharova / Spanish ✗ (Czech)\n",
      "44500 4% (1m 34s) 1.1597 Wong / Chinese ✓\n",
      "45000 4% (1m 35s) 1.5775 Montana / Spanish ✗ (Italian)\n",
      "45500 4% (1m 36s) 1.8050 Faucher / Scottish ✗ (French)\n",
      "46000 4% (1m 36s) 2.0327 Alliott / German ✗ (English)\n",
      "46500 4% (1m 37s) 2.7375 Conrad / Portuguese ✗ (English)\n",
      "47000 4% (1m 38s) 2.0131 Schenck / Czech ✗ (German)\n",
      "47500 4% (1m 38s) 3.2015 Taggart / Arabic ✗ (English)\n",
      "48000 4% (1m 39s) 2.9026 O'Hara / Japanese ✗ (Irish)\n",
      "48500 4% (1m 40s) 1.2486 Cao / Chinese ✗ (Vietnamese)\n",
      "49000 4% (1m 40s) 1.6786 Hiro / Portuguese ✗ (Japanese)\n",
      "49500 4% (1m 41s) 1.6053 Delgado / Portuguese ✓\n",
      "50000 5% (1m 41s) 3.6251 Peary / Portuguese ✗ (Czech)\n",
      "50500 5% (1m 42s) 0.5787 Stamatelos / Greek ✓\n",
      "51000 5% (1m 43s) 4.1022 Paulis / Greek ✗ (Dutch)\n",
      "51500 5% (1m 44s) 0.3393 Hadjiyianakies / Greek ✓\n",
      "52000 5% (1m 44s) 3.7150 Salazar / Arabic ✗ (Spanish)\n",
      "52500 5% (1m 45s) 1.8294 Firmin / French ✓\n",
      "53000 5% (1m 45s) 0.4229 Chellos / Greek ✓\n",
      "53500 5% (1m 46s) 1.0237 Cha / Korean ✓\n",
      "54000 5% (1m 47s) 2.0663 Kentaro / Portuguese ✗ (Japanese)\n",
      "54500 5% (1m 47s) 3.1670 Adam / Arabic ✗ (Irish)\n",
      "55000 5% (1m 48s) 0.4193 Negrini / Italian ✓\n",
      "55500 5% (1m 49s) 1.0469 Spano / Italian ✓\n",
      "56000 5% (1m 49s) 1.1646 Phi / Korean ✗ (Vietnamese)\n",
      "56500 5% (1m 50s) 4.9398 Tos / Korean ✗ (Spanish)\n",
      "57000 5% (1m 51s) 1.3954 Sherak / Polish ✗ (Czech)\n",
      "57500 5% (1m 51s) 1.2546 Kools / Dutch ✓\n",
      "58000 5% (1m 52s) 5.0909 Mcmillan / Scottish ✗ (English)\n",
      "58500 5% (1m 53s) 2.3152 Andel / Spanish ✗ (Dutch)\n",
      "59000 5% (1m 54s) 1.6723 Purcell / Italian ✗ (English)\n",
      "59500 5% (1m 54s) 0.1440 Wronski / Polish ✓\n",
      "60000 6% (1m 55s) 0.7622 Lin / Chinese ✓\n",
      "60500 6% (1m 56s) 1.2951 Kaplanek / Polish ✗ (Czech)\n",
      "61000 6% (1m 56s) 1.3821 Bernat / German ✓\n",
      "61500 6% (1m 57s) 0.1401 Son / Korean ✓\n",
      "62000 6% (1m 58s) 0.0871 Jaskolski / Polish ✓\n",
      "62500 6% (1m 58s) 1.8021 Roma / Italian ✗ (Spanish)\n",
      "63000 6% (1m 59s) 0.5272 Takewaki / Japanese ✓\n",
      "63500 6% (2m 0s) 0.6758 Yun / Korean ✓\n",
      "64000 6% (2m 1s) 0.0040 Manoukarakis / Greek ✓\n",
      "64500 6% (2m 3s) 3.3370 Paris / Greek ✗ (French)\n",
      "65000 6% (2m 3s) 2.1413 Gajos / Greek ✗ (Polish)\n",
      "65500 6% (2m 4s) 0.0220 Moraitopoulos / Greek ✓\n",
      "66000 6% (2m 5s) 1.0956 Tian / Vietnamese ✗ (Chinese)\n",
      "66500 6% (2m 6s) 1.3578 Prinsen / Dutch ✓\n",
      "67000 6% (2m 6s) 0.0483 Dubnyakov / Russian ✓\n",
      "67500 6% (2m 7s) 2.4153 Carey / Spanish ✗ (Irish)\n",
      "68000 6% (2m 7s) 0.7035 Shananykin / Russian ✓\n",
      "68500 6% (2m 8s) 1.3112 Biermann / German ✓\n",
      "69000 6% (2m 9s) 3.4881 Hana / Korean ✗ (Czech)\n",
      "69500 6% (2m 9s) 2.1765 Mellor / Scottish ✗ (English)\n",
      "70000 7% (2m 10s) 0.4044 Ta / Vietnamese ✓\n",
      "70500 7% (2m 11s) 0.3686 Mckenzie / Scottish ✓\n",
      "71000 7% (2m 11s) 2.8643 Asturias / Greek ✗ (Spanish)\n",
      "71500 7% (2m 12s) 1.4353 Pinho / Japanese ✗ (Portuguese)\n",
      "72000 7% (2m 12s) 3.1183 Harries / Portuguese ✗ (English)\n",
      "72500 7% (2m 13s) 1.0667 Ha / Korean ✗ (Vietnamese)\n",
      "73000 7% (2m 13s) 0.2437 Shon / Korean ✓\n",
      "73500 7% (2m 14s) 0.1543 Quyen / Vietnamese ✓\n",
      "74000 7% (2m 15s) 2.3159 Grimshaw / Czech ✗ (English)\n",
      "74500 7% (2m 15s) 0.2030 Gomatos / Greek ✓\n",
      "75000 7% (2m 16s) 2.6071 Gouveia / Spanish ✗ (Portuguese)\n",
      "75500 7% (2m 17s) 0.8963 Abbiati / Japanese ✗ (Italian)\n",
      "76000 7% (2m 17s) 0.8674 Tian / Vietnamese ✗ (Chinese)\n",
      "76500 7% (2m 18s) 2.9447 Chung / Korean ✗ (Vietnamese)\n",
      "77000 7% (2m 18s) 2.5198 Piper / German ✗ (English)\n",
      "77500 7% (2m 19s) 1.1763 Tadhg / Irish ✓\n",
      "78000 7% (2m 20s) 0.0159 Kikuchi / Japanese ✓\n",
      "78500 7% (2m 20s) 1.5008 Solomon / French ✓\n",
      "79000 7% (2m 21s) 0.6787 An / Vietnamese ✓\n",
      "79500 7% (2m 21s) 0.1345 Sartini / Italian ✓\n",
      "80000 8% (2m 22s) 1.5329 Kang / Korean ✗ (Chinese)\n",
      "80500 8% (2m 23s) 0.5295 Kolen / Dutch ✓\n",
      "81000 8% (2m 23s) 0.9136 Gomes / Portuguese ✓\n",
      "81500 8% (2m 24s) 0.8057 Rompaey / Dutch ✓\n",
      "82000 8% (2m 24s) 1.8414 Garcia / Spanish ✗ (Portuguese)\n",
      "82500 8% (2m 25s) 1.5525 Obando / Italian ✗ (Spanish)\n",
      "83000 8% (2m 25s) 1.5095 Aquila / Spanish ✗ (Italian)\n",
      "83500 8% (2m 26s) 1.9299 Grant / Vietnamese ✗ (Scottish)\n",
      "84000 8% (2m 27s) 0.9951 Sarraf / Arabic ✓\n",
      "84500 8% (2m 27s) 3.0481 Richard / Scottish ✗ (French)\n",
      "85000 8% (2m 28s) 3.9687 Aitken / Dutch ✗ (Scottish)\n",
      "85500 8% (2m 29s) 0.2719 Bukoski / Polish ✓\n",
      "86000 8% (2m 29s) 2.2676 Salazar / Czech ✗ (Portuguese)\n",
      "86500 8% (2m 30s) 0.0579 Naoimhin / Irish ✓\n",
      "87000 8% (2m 31s) 2.3857 Loewe / French ✗ (German)\n",
      "87500 8% (2m 31s) 1.3358 Ngai / Chinese ✗ (Korean)\n",
      "88000 8% (2m 32s) 2.2569 Aitken / Dutch ✗ (Scottish)\n",
      "88500 8% (2m 33s) 0.6006 Coelho / Portuguese ✓\n",
      "89000 8% (2m 33s) 1.3112 Sarto / Portuguese ✗ (Italian)\n",
      "89500 8% (2m 34s) 0.8779 White / Scottish ✓\n",
      "90000 9% (2m 34s) 1.0508 Cabral / Portuguese ✓\n",
      "90500 9% (2m 35s) 0.9617 Hanek / Czech ✓\n",
      "91000 9% (2m 36s) 0.8773 Cao / Vietnamese ✓\n",
      "91500 9% (2m 36s) 1.3544 Burns / Scottish ✓\n",
      "92000 9% (2m 37s) 1.0271 Villalobos / Spanish ✓\n",
      "92500 9% (2m 37s) 1.7011 Battaglia / Spanish ✗ (Italian)\n",
      "93000 9% (2m 38s) 0.5362 Ryu / Korean ✓\n",
      "93500 9% (2m 39s) 5.1090 Haanrade / Irish ✗ (Dutch)\n",
      "94000 9% (2m 39s) 1.9093 Aitken / Dutch ✗ (Scottish)\n",
      "94500 9% (2m 40s) 2.2870 Elena / Spanish ✗ (Italian)\n",
      "95000 9% (2m 41s) 0.6188 Yang / Chinese ✓\n",
      "95500 9% (2m 41s) 3.1063 King / Chinese ✗ (Scottish)\n",
      "96000 9% (2m 42s) 0.4505 Hao / Chinese ✓\n",
      "96500 9% (2m 42s) 0.2097 Coelho / Portuguese ✓\n",
      "97000 9% (2m 43s) 1.3444 Maria / Portuguese ✗ (Spanish)\n",
      "97500 9% (2m 43s) 2.9547 Araya / Arabic ✗ (Spanish)\n",
      "98000 9% (2m 44s) 0.8197 Marek / Polish ✓\n",
      "98500 9% (2m 45s) 1.5639 Lim / Korean ✗ (Chinese)\n",
      "99000 9% (2m 45s) 3.8512 Chung / Korean ✗ (Vietnamese)\n",
      "99500 9% (2m 46s) 1.1719 Horn / Dutch ✓\n",
      "100000 10% (2m 46s) 0.5563 Riain / Irish ✓\n",
      "100500 10% (2m 47s) 0.5710 Sarraf / Arabic ✓\n",
      "101000 10% (2m 48s) 0.7472 Bonhomme / French ✓\n",
      "101500 10% (2m 48s) 0.6729 Johnston / Scottish ✓\n",
      "102000 10% (2m 49s) 0.0309 Gwang  / Korean ✓\n",
      "102500 10% (2m 49s) 1.7803 Tuckey / Scottish ✗ (English)\n",
      "103000 10% (2m 50s) 1.4451 Norcross / English ✓\n",
      "103500 10% (2m 50s) 0.1461 Kanavos / Greek ✓\n",
      "104000 10% (2m 51s) 1.7520 Paranin / French ✗ (Russian)\n",
      "104500 10% (2m 52s) 0.4065 Aucciello / Italian ✓\n",
      "105000 10% (2m 52s) 0.3346 D'cruze / Portuguese ✓\n",
      "105500 10% (2m 53s) 0.3054 Piterskikh / Russian ✓\n",
      "106000 10% (2m 53s) 0.4097 Ablesimoff / Russian ✓\n",
      "106500 10% (2m 54s) 1.9432 Larenz / Spanish ✗ (German)\n",
      "107000 10% (2m 55s) 1.3248 Kelly / English ✗ (Scottish)\n",
      "107500 10% (2m 55s) 2.5504 Kwei / Korean ✗ (Chinese)\n",
      "108000 10% (2m 56s) 1.0182 Nazari / Japanese ✗ (Arabic)\n",
      "108500 10% (2m 56s) 0.2478 Sayegh / Arabic ✓\n",
      "109000 10% (2m 57s) 1.2325 Bonnet / French ✓\n",
      "109500 10% (2m 58s) 3.8802 Haanrath / Scottish ✗ (Dutch)\n",
      "110000 11% (2m 58s) 2.6076 Wawrzaszek / Czech ✗ (Polish)\n",
      "110500 11% (2m 59s) 2.0342 Klimek / Czech ✗ (Polish)\n",
      "111000 11% (2m 59s) 0.8078 Colman / Irish ✓\n",
      "111500 11% (3m 0s) 0.0353 Sokolowski / Polish ✓\n",
      "112000 11% (3m 1s) 0.5234 Favreau / French ✓\n",
      "112500 11% (3m 1s) 2.9087 Bhrighde / Japanese ✗ (Irish)\n",
      "113000 11% (3m 2s) 2.3626 Kofron / Dutch ✗ (Czech)\n",
      "113500 11% (3m 2s) 0.5966 Zogby / Arabic ✓\n",
      "114000 11% (3m 3s) 2.3129 Sleiman / Irish ✗ (Arabic)\n",
      "114500 11% (3m 4s) 0.6175 Sebastiani / Italian ✓\n",
      "115000 11% (3m 4s) 2.1344 Bando / Spanish ✗ (Japanese)\n",
      "115500 11% (3m 5s) 2.8233 Maneates / Portuguese ✗ (Greek)\n",
      "116000 11% (3m 5s) 3.0822 Leon / Korean ✗ (French)\n",
      "116500 11% (3m 6s) 0.3627 Mcdonald / Scottish ✓\n",
      "117000 11% (3m 6s) 2.2496 Specht / German ✗ (Dutch)\n",
      "117500 11% (3m 7s) 0.2852 O'Rourke / Irish ✓\n",
      "118000 11% (3m 8s) 1.2605 Savatier / French ✓\n",
      "118500 11% (3m 8s) 3.7476 Maas / Portuguese ✗ (Dutch)\n",
      "119000 11% (3m 9s) 2.6172 Togo / Vietnamese ✗ (Japanese)\n",
      "119500 11% (3m 10s) 0.0502 Metrofanis / Greek ✓\n",
      "120000 12% (3m 10s) 0.0471 Demakis / Greek ✓\n",
      "120500 12% (3m 11s) 0.1486 Mcdonald / Scottish ✓\n",
      "121000 12% (3m 14s) 1.0032 Nowak / Polish ✓\n",
      "121500 12% (3m 17s) 2.1432 Fairbrother / Russian ✗ (English)\n",
      "122000 12% (3m 19s) 0.2392 Drivakis / Greek ✓\n",
      "122500 12% (3m 20s) 1.3402 Jackson / Russian ✗ (Scottish)\n",
      "123000 12% (3m 21s) 0.1353 Golovatsky / Russian ✓\n",
      "123500 12% (3m 23s) 1.4616 Anderson / Scottish ✗ (English)\n",
      "124000 12% (3m 24s) 1.9867 Plourde / English ✗ (French)\n",
      "124500 12% (3m 25s) 3.9137 Makhlai / Japanese ✗ (Russian)\n",
      "125000 12% (3m 27s) 0.7439 Seelen / Dutch ✓\n",
      "125500 12% (3m 28s) 0.4918 Gonzales / Spanish ✓\n",
      "126000 12% (3m 29s) 0.3513 Serafim / Portuguese ✓\n",
      "126500 12% (3m 32s) 1.4441 Kelly / English ✗ (Scottish)\n",
      "127000 12% (3m 34s) 1.3113 Kloet / Dutch ✓\n",
      "127500 12% (3m 37s) 0.0159 Jakushev / Russian ✓\n",
      "128000 12% (3m 40s) 0.7874 Lew / Chinese ✓\n",
      "128500 12% (3m 42s) 0.2827 Van / Vietnamese ✓\n",
      "129000 12% (3m 43s) 0.0229 Mogilnikov / Russian ✓\n",
      "129500 12% (3m 46s) 0.9641 Romero / Italian ✓\n",
      "130000 13% (3m 47s) 0.2063 Henderson / Scottish ✓\n",
      "130500 13% (3m 49s) 1.3885 Alescio / Portuguese ✗ (Italian)\n",
      "131000 13% (3m 51s) 0.1367 Hyun  / Korean ✓\n",
      "131500 13% (3m 53s) 1.7675 Cote / English ✗ (French)\n",
      "132000 13% (3m 55s) 1.8680 De la fontaine / English ✗ (French)\n",
      "132500 13% (3m 57s) 0.8277 Patril / Czech ✓\n",
      "133000 13% (3m 58s) 0.2254 Tableriou / Greek ✓\n",
      "133500 13% (4m 1s) 0.6972 Szewc / Polish ✓\n",
      "134000 13% (4m 4s) 2.6016 Blazejovsky / Russian ✗ (Czech)\n",
      "134500 13% (4m 6s) 2.6836 Dempster / Scottish ✗ (English)\n",
      "135000 13% (4m 8s) 2.4165 Ruba / Spanish ✗ (Czech)\n",
      "135500 13% (4m 11s) 1.7020 Shui / Korean ✗ (Chinese)\n",
      "136000 13% (4m 14s) 0.0052 Tikhobrazov / Russian ✓\n",
      "136500 13% (4m 17s) 0.0875 Gavrilopoulos / Greek ✓\n",
      "137000 13% (4m 19s) 0.5733 Bursinos / Greek ✓\n",
      "137500 13% (4m 21s) 0.3999 Stevenson / Scottish ✓\n",
      "138000 13% (4m 22s) 0.7351 Schwarzenegger / German ✓\n",
      "138500 13% (4m 25s) 3.1011 Lunt / French ✗ (English)\n",
      "139000 13% (4m 29s) 0.4663 Paredes / Portuguese ✓\n",
      "139500 13% (4m 32s) 0.0447 O'Brien / Irish ✓\n",
      "140000 14% (4m 35s) 0.3132 Kasprzak / Polish ✓\n",
      "140500 14% (4m 39s) 0.5820 Malihoudis / Greek ✓\n",
      "141000 14% (4m 42s) 1.2462 Brian / Scottish ✗ (Irish)\n",
      "141500 14% (4m 45s) 3.2208 Costa / Spanish ✗ (Portuguese)\n",
      "142000 14% (4m 50s) 0.7678 Bosko / Polish ✓\n",
      "142500 14% (4m 54s) 0.6205 Gaertner / German ✓\n",
      "143000 14% (4m 58s) 0.7167 Kijmuta / Japanese ✓\n",
      "143500 14% (5m 2s) 1.6678 Franco / Italian ✗ (Spanish)\n",
      "144000 14% (5m 5s) 0.4354 Vourlis / Greek ✓\n",
      "144500 14% (5m 8s) 0.3082 Apsley / English ✓\n",
      "145000 14% (5m 10s) 2.7737 Labriola / Spanish ✗ (Italian)\n",
      "145500 14% (5m 14s) 2.1428 Macias / Portuguese ✗ (Spanish)\n",
      "146000 14% (5m 17s) 2.8346 Shaw / Chinese ✗ (Scottish)\n",
      "146500 14% (5m 18s) 1.0258 Alexander / Scottish ✓\n",
      "147000 14% (5m 21s) 2.7410 Aodha / Arabic ✗ (Irish)\n",
      "147500 14% (5m 23s) 1.3787 Roma / Italian ✗ (Spanish)\n",
      "148000 14% (5m 25s) 0.2226 Dertilis / Greek ✓\n",
      "148500 14% (5m 27s) 0.3515 O'Mahoney / Irish ✓\n",
      "149000 14% (5m 29s) 1.7862 Homatsky / Scottish ✗ (Russian)\n",
      "149500 14% (5m 32s) 0.5493 Suh / Korean ✓\n",
      "150000 15% (5m 35s) 1.3139 Lawler / German ✗ (English)\n",
      "150500 15% (5m 37s) 1.0282 Kobi / Arabic ✗ (Japanese)\n",
      "151000 15% (5m 39s) 2.1075 Ubina / Japanese ✗ (Spanish)\n",
      "151500 15% (5m 40s) 1.3174 Chong / Chinese ✗ (Korean)\n",
      "152000 15% (5m 43s) 0.6952 Machado / Portuguese ✓\n",
      "152500 15% (5m 45s) 3.0975 De leon / Scottish ✗ (Spanish)\n",
      "153000 15% (5m 46s) 0.5004 Rim / Korean ✓\n",
      "153500 15% (5m 48s) 1.2980 Hintzen / German ✓\n",
      "154000 15% (5m 50s) 0.8379 Madden / Irish ✓\n",
      "154500 15% (5m 53s) 2.0599 Fuentes / Portuguese ✗ (Spanish)\n",
      "155000 15% (5m 55s) 2.2131 Houtum / Arabic ✗ (Dutch)\n",
      "155500 15% (5m 57s) 0.0397 Doikov / Russian ✓\n",
      "156000 15% (5m 59s) 0.1419 Armani / Italian ✓\n",
      "156500 15% (6m 2s) 1.0717 Cabral / Portuguese ✓\n",
      "157000 15% (6m 4s) 1.6706 Varey / French ✗ (English)\n",
      "157500 15% (6m 7s) 0.4014 Vlasak / Czech ✓\n",
      "158000 15% (6m 9s) 0.2070 Paterson / Scottish ✓\n",
      "158500 15% (6m 11s) 0.3274 Sevriens / Dutch ✓\n",
      "159000 15% (6m 14s) 0.4379 Dunajski / Polish ✓\n",
      "159500 15% (6m 16s) 0.9566 Longley / French ✗ (English)\n",
      "160000 16% (6m 18s) 0.8482 Deforest / French ✓\n",
      "160500 16% (6m 20s) 0.5570 Castro / Portuguese ✓\n",
      "161000 16% (6m 22s) 1.2635 Kenzel / Czech ✓\n",
      "161500 16% (6m 24s) 0.1180 Ta / Vietnamese ✓\n",
      "162000 16% (6m 26s) 0.0032 O'Mahony / Irish ✓\n",
      "162500 16% (6m 31s) 0.6872 Youn / Korean ✓\n",
      "163000 16% (6m 36s) 0.6060 Fei / Chinese ✓\n",
      "163500 16% (6m 39s) 0.2049 Vo / Vietnamese ✓\n",
      "164000 16% (6m 41s) 2.1973 Chou / Korean ✗ (Chinese)\n",
      "164500 16% (6m 44s) 0.0216 Matsoukis / Greek ✓\n",
      "165000 16% (6m 48s) 1.2259 Abbatantuono / Spanish ✗ (Italian)\n",
      "165500 16% (6m 50s) 0.2873 Kaminski / Polish ✓\n",
      "166000 16% (6m 53s) 0.0244 Higashikuni / Japanese ✓\n",
      "166500 16% (6m 55s) 2.3095 Nevins / Greek ✗ (English)\n",
      "167000 16% (6m 58s) 0.0869 Janimov / Russian ✓\n",
      "167500 16% (7m 1s) 0.1215 Haddad / Arabic ✓\n",
      "168000 16% (7m 4s) 0.8067 Proulx / French ✓\n",
      "168500 16% (7m 7s) 0.5353 Amari / Arabic ✓\n",
      "169000 16% (7m 9s) 1.3476 Pierre / Dutch ✗ (French)\n",
      "169500 16% (7m 11s) 0.0039 Niemczyk / Polish ✓\n",
      "170000 17% (7m 13s) 0.7039 Chew / Chinese ✓\n",
      "170500 17% (7m 15s) 1.1557 Robles / Spanish ✓\n",
      "171000 17% (7m 18s) 0.2956 Hew / Chinese ✓\n",
      "171500 17% (7m 21s) 0.2643 D'cruze / Portuguese ✓\n",
      "172000 17% (7m 23s) 0.9657 Fabron / French ✓\n",
      "172500 17% (7m 25s) 0.1691 Kasprzak / Polish ✓\n",
      "173000 17% (7m 27s) 2.6736 Gerald / Scottish ✗ (Irish)\n",
      "173500 17% (7m 29s) 0.2612 Sarraf / Arabic ✓\n",
      "174000 17% (7m 31s) 1.6831 Denend / French ✗ (Dutch)\n",
      "174500 17% (7m 33s) 0.3026 Everhart / German ✓\n",
      "175000 17% (7m 35s) 3.1119 Arthur / Arabic ✗ (French)\n",
      "175500 17% (7m 37s) 0.5275 D'cruz / Portuguese ✓\n",
      "176000 17% (7m 38s) 0.0737 Nicolai / Italian ✓\n",
      "176500 17% (7m 41s) 0.1623 Mushashibo / Japanese ✓\n",
      "177000 17% (7m 43s) 0.1403 Sook / Korean ✓\n",
      "177500 17% (7m 46s) 2.3756 Lawa / Korean ✗ (Czech)\n",
      "178000 17% (7m 48s) 1.6056 Milligan / French ✗ (Irish)\n",
      "178500 17% (7m 50s) 0.1770 Vilkitsky / Russian ✓\n",
      "179000 17% (7m 52s) 0.6385 Antoun / Arabic ✓\n",
      "179500 17% (7m 54s) 0.2823 Kouros / Greek ✓\n",
      "180000 18% (7m 55s) 0.6355 Issa / Arabic ✓\n",
      "180500 18% (7m 57s) 1.9356 Winograd / Irish ✗ (Polish)\n",
      "181000 18% (7m 59s) 1.6723 Picasso / Italian ✗ (Spanish)\n",
      "181500 18% (8m 1s) 4.6322 Jordan / Polish ✗ (French)\n",
      "182000 18% (8m 4s) 0.3930 Phi / Vietnamese ✓\n",
      "182500 18% (8m 7s) 1.2964 Woodley / Scottish ✗ (English)\n",
      "183000 18% (8m 10s) 2.6086 Pinter / French ✗ (Czech)\n",
      "183500 18% (8m 12s) 0.0205 Giannakos / Greek ✓\n",
      "184000 18% (8m 15s) 0.5806 Ibuka / Japanese ✓\n",
      "184500 18% (8m 20s) 1.1620 Serak / Polish ✗ (Czech)\n",
      "185000 18% (8m 23s) 1.1954 Spannagel / German ✓\n",
      "185500 18% (8m 26s) 0.4660 Arriola / Spanish ✓\n",
      "186000 18% (8m 28s) 0.9176 Madeira / Portuguese ✓\n",
      "186500 18% (8m 31s) 0.0094 Niemczyk / Polish ✓\n",
      "187000 18% (8m 32s) 1.0952 Kerper / Dutch ✗ (German)\n",
      "187500 18% (8m 35s) 3.2194 Campbell / Dutch ✗ (Scottish)\n",
      "188000 18% (8m 37s) 0.1449 Guan / Chinese ✓\n",
      "188500 18% (8m 39s) 0.5111 Close / Greek ✓\n",
      "189000 18% (8m 41s) 0.4866 Palmeiro / Portuguese ✓\n",
      "189500 18% (8m 43s) 0.7705 Suh / Korean ✓\n",
      "190000 19% (8m 45s) 0.5103 Mo / Korean ✓\n",
      "190500 19% (8m 47s) 1.9513 Han / Chinese ✗ (Korean)\n",
      "191000 19% (8m 50s) 0.0021 Panayiotopoulos / Greek ✓\n",
      "191500 19% (8m 53s) 1.1880 Pho / Vietnamese ✓\n",
      "192000 19% (8m 55s) 0.6784 Tapia / Spanish ✓\n",
      "192500 19% (8m 58s) 3.2040 Schallom / Scottish ✗ (Czech)\n",
      "193000 19% (9m 1s) 0.1022 Vamvakidis / Greek ✓\n",
      "193500 19% (9m 4s) 1.2711 Slepica / Czech ✓\n",
      "194000 19% (9m 7s) 1.3251 Mullen / Irish ✓\n",
      "194500 19% (9m 9s) 0.5911 D'aramitz / French ✓\n",
      "195000 19% (9m 10s) 4.1178 Thorndyke / Russian ✗ (English)\n",
      "195500 19% (9m 12s) 0.0087 O'Neal / Irish ✓\n",
      "196000 19% (9m 13s) 4.1871 Lu / Vietnamese ✗ (Chinese)\n",
      "196500 19% (9m 14s) 0.0078 Yokokawa / Japanese ✓\n",
      "197000 19% (9m 17s) 0.1250 Stevenson / Scottish ✓\n",
      "197500 19% (9m 20s) 3.3960 Soler / German ✗ (Spanish)\n",
      "198000 19% (9m 23s) 1.5464 Mendel / Dutch ✗ (German)\n",
      "198500 19% (9m 27s) 0.5719 Savchak / Czech ✓\n",
      "199000 19% (9m 29s) 1.0272 Motoori / Italian ✗ (Japanese)\n",
      "199500 19% (9m 30s) 0.0116 Karkampasis / Greek ✓\n",
      "200000 20% (9m 31s) 0.7419 Ngai / Korean ✓\n",
      "200500 20% (9m 32s) 1.5827 Subertova / Italian ✗ (Czech)\n",
      "201000 20% (9m 34s) 0.2425 Peng / Chinese ✓\n",
      "201500 20% (9m 35s) 1.1841 Santos / Portuguese ✓\n",
      "202000 20% (9m 36s) 0.7359 Starek / Polish ✓\n",
      "202500 20% (9m 37s) 2.6552 Ross / German ✗ (Scottish)\n",
      "203000 20% (9m 39s) 3.5566 Han / Chinese ✗ (Korean)\n",
      "203500 20% (9m 40s) 0.0819 Piontek / Polish ✓\n",
      "204000 20% (9m 41s) 2.0728 Meindl / Irish ✗ (German)\n",
      "204500 20% (9m 41s) 1.9381 Salazar / Portuguese ✗ (Spanish)\n",
      "205000 20% (9m 42s) 1.4097 Crespo / Italian ✗ (Portuguese)\n",
      "205500 20% (9m 43s) 0.0167 Fujimoto / Japanese ✓\n",
      "206000 20% (9m 44s) 0.4094 Zhou / Chinese ✓\n",
      "206500 20% (9m 45s) 0.2077 Gibson / Scottish ✓\n",
      "207000 20% (9m 46s) 0.5015 Fournier / French ✓\n",
      "207500 20% (9m 47s) 2.3543 Shaw / Chinese ✗ (Scottish)\n",
      "208000 20% (9m 48s) 2.0911 Munro / Italian ✗ (Scottish)\n",
      "208500 20% (9m 50s) 0.6636 Yamanouchi / Japanese ✓\n",
      "209000 20% (9m 50s) 0.4713 Houten / Dutch ✓\n",
      "209500 20% (9m 51s) 2.3790 Soukup / Dutch ✗ (Czech)\n",
      "210000 21% (9m 53s) 1.7143 Obando / Italian ✗ (Spanish)\n",
      "210500 21% (9m 54s) 1.2052 Dai / Vietnamese ✗ (Chinese)\n",
      "211000 21% (9m 54s) 1.3285 Tovstukha / Japanese ✗ (Russian)\n",
      "211500 21% (9m 55s) 1.4212 Allman / Irish ✗ (English)\n",
      "212000 21% (9m 56s) 1.1080 Yeo / Chinese ✗ (Korean)\n",
      "212500 21% (9m 57s) 1.5642 Brisbois / Spanish ✗ (French)\n",
      "213000 21% (9m 58s) 2.4526 Mulder / Scottish ✗ (Dutch)\n",
      "213500 21% (9m 59s) 1.1290 Shibata / Portuguese ✗ (Japanese)\n",
      "214000 21% (10m 0s) 3.8940 Althuis / Greek ✗ (Dutch)\n",
      "214500 21% (10m 1s) 0.2704 Wasem / Arabic ✓\n",
      "215000 21% (10m 2s) 3.1265 Gehrig / Scottish ✗ (German)\n",
      "215500 21% (10m 3s) 0.2311 Bradan / Irish ✓\n",
      "216000 21% (10m 4s) 0.6250 Autenburg / German ✓\n",
      "216500 21% (10m 5s) 1.0482 Chin / Korean ✓\n",
      "217000 21% (10m 6s) 0.4797 Murray / Scottish ✓\n",
      "217500 21% (10m 7s) 1.2097 Pasternak / Polish ✓\n",
      "218000 21% (10m 8s) 3.4334 Mai / Chinese ✗ (Vietnamese)\n",
      "218500 21% (10m 9s) 0.9946 Chu / Chinese ✗ (Vietnamese)\n",
      "219000 21% (10m 9s) 0.1250 Haanraats / Dutch ✓\n",
      "219500 21% (10m 11s) 2.4114 Raizman / Irish ✗ (Russian)\n",
      "220000 22% (10m 12s) 0.2920 Paitakes / Greek ✓\n",
      "220500 22% (10m 13s) 0.2346 Bazhutkin / Russian ✓\n",
      "221000 22% (10m 14s) 0.6961 Seo / Korean ✓\n",
      "221500 22% (10m 16s) 0.2338 Schneiders / Dutch ✓\n",
      "222000 22% (10m 17s) 3.0444 Richard / Scottish ✗ (Dutch)\n",
      "222500 22% (10m 18s) 0.0180 Frangopoulos / Greek ✓\n",
      "223000 22% (10m 19s) 0.5649 Miazga / Polish ✓\n",
      "223500 22% (10m 20s) 0.4621 Cavallo / Italian ✓\n",
      "224000 22% (10m 22s) 0.2774 Berardi / Italian ✓\n",
      "224500 22% (10m 23s) 2.2846 Maciomhair / Greek ✗ (Irish)\n",
      "225000 22% (10m 24s) 2.9947 Lowe / English ✗ (German)\n",
      "225500 22% (10m 27s) 0.0104 Winogrodzki / Polish ✓\n",
      "226000 22% (10m 29s) 0.7210 Medeiros / Portuguese ✓\n",
      "226500 22% (10m 30s) 1.4545 Rompu / French ✗ (Dutch)\n",
      "227000 22% (10m 31s) 1.0009 Shalhoub / Arabic ✓\n",
      "227500 22% (10m 32s) 0.1504 Okura / Japanese ✓\n",
      "228000 22% (10m 33s) 1.5023 Bello / Italian ✗ (Spanish)\n",
      "228500 22% (10m 34s) 3.0697 Neville / Italian ✗ (Irish)\n",
      "229000 22% (10m 36s) 1.3533 Gray / Scottish ✓\n",
      "229500 22% (10m 37s) 0.3909 Phi / Vietnamese ✓\n",
      "230000 23% (10m 38s) 2.5073 Nekuza / Arabic ✗ (Czech)\n",
      "230500 23% (10m 39s) 0.5271 Momotami / Japanese ✓\n",
      "231000 23% (10m 40s) 0.0398 Bencivenni / Italian ✓\n",
      "231500 23% (10m 41s) 2.5834 Mas / Chinese ✗ (Spanish)\n",
      "232000 23% (10m 43s) 0.0293 Stamatelos / Greek ✓\n",
      "232500 23% (10m 45s) 1.7286 Jez / Chinese ✗ (Polish)\n",
      "233000 23% (10m 47s) 0.0681 Mcdonald / Scottish ✓\n",
      "233500 23% (10m 48s) 0.5638 Nader / Arabic ✓\n",
      "234000 23% (10m 50s) 0.0122 Carracci / Italian ✓\n",
      "234500 23% (10m 51s) 0.5198 Oliver / Spanish ✓\n",
      "235000 23% (10m 52s) 2.9348 Portoghese / English ✗ (Italian)\n",
      "235500 23% (10m 53s) 1.2749 Espinosa / Greek ✗ (Spanish)\n",
      "236000 23% (10m 55s) 1.2910 Hill / Scottish ✓\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-87018d6a3f33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomTrainingExample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-cc30a2d508fa>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(category_tensor, line_tensor)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Add parameters' gradients to their values, multiplied by learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "importlib.reload(utils)\n",
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 200000\n",
    "print_every = 500\n",
    "plot_every = 10000\n",
    "\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As ONNX "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dummy_input = lineToTensor('Jones')[0]\n",
    "hidden = rnn.initHidden()\n",
    "\n",
    "torch.onnx.export(model=rnn,\n",
    "                  args=(dummy_input, hidden), \n",
    "                  f=str(PurePath(bundle_root, 'common/char-rnn-classification.onnx')),\n",
    "                  verbose=True,\n",
    "                  input_names=['name'], output_names=['origin'])\n",
    "\n",
    "### As torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(rnn, str(PurePath(bundle_root, 'common/char-rnn-classification.pth')))\n",
    "torch.save(rnn.state_dict(), str(PurePath(bundle_root, 'common/char-rnn-classification.pt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As tensorflow lite (smallest size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "import tensorflow as tf\n",
    "model = prepare(onnx.load(str(PurePath(bundle_root, 'common/char-rnn-classification.onnx'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read with Caffe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install caffe2\n",
    "import caffe2.python.onnx.backend as backend\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the Results\n",
    "--------------------\n",
    "\n",
    "Plotting the historical loss from ``all_losses`` shows the network\n",
    "learning:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f88f43eef60>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAHixJREFUeJzt3Xl0HOWd7vFv9aK1W5Zt2S1Zki3ZWBbGeDfGC2BC2CHBCZAELgQCYyCeBHJyZ5KbuTNkJvfOTO49IUzuBIPZMwMkgzHLkIR1wAtewDLe9w1bsiVZlrWvra77R5WNbMtWy26ruqufzzl1urr6pfWjT/vRq7feesswTRMREXEXj9MFiIhI7CncRURcSOEuIuJCCncRERdSuIuIuJDCXUTEhRTuIiIuFE24FwIfAVuAzcDDp2k3B1hnt1kSi+JEROTsGFFcxJRnb2uBIFAG3IIV9sdkAyuA64D9wFCgOtbFiohIdHxRtDlkbwCNwFYgnxPD/Q5gMVawQxTBnpOTYxYVFUVdqIiIQFlZWY1pmkN6axdNuHdXBEwCVp90vATwAx9j9e7/BfhdD//9PHujqKiINWvW9PHHi4gkN8MwvoimXV/CPQC8BjwCNPTwPlOAq4B0YCWwCthxUruF9gagRW1ERM6TaMPdjxXsL2ENv5ysHDgCNNvbUmACp4a7iIj0g2hmyxjAs1hj7Y+dps2bwGysXxYZwHS7vYiIOCCanvss4C5gI9ZUR4CfAcPt/SexgvwdYAMQAZ4BNsW0UhERiVo04b4cq/fem/9rbyIi4jBdoSoi4kIKdxERF0q4cN9W2cA//XkrTe1hp0sREYlbCRfu5bWtPLVkD9srG50uRUQkbiVcuI/JDQKwo0rhLiJyOgkX7vnZ6WSmeNVzFxE5g4QLd4/HoCQ3yLbKk1dAEBGRYxIu3AHGhIJsr2wkiuWKRUSSUmKGe26Qoy2dHG5qd7oUEZG4lJjhHrJPqlY2OVyJiEh8Ssxwt2fMaNxdRKRnCRnugwOp5ARSNB1SROQ0EjLcweq9azqkiEjPEjfcQ1nsqGoiEtGMGRGRkyVuuOcGaO3s4sDRFqdLERGJOwkc7lkAbNPQjIjIKRI23EcPDQCwQ+EuInKKhA33zFQfwwdlsE0zZkRETpGw4Q5QEgqq5y4i0oNowr0Q+AjYAmwGHj5D22lAGLj13EvrXWlukD01zbSHu/rjx4mIJIxowj0M/BgYC1wKzLf3T+YFfgm8F7PqejEmN0hXxGTP4eb++pEiIgkhmnA/BKy19xuBrUB+D+1+ALwGVMemtN4dW4ZAFzOJiJyor2PuRcAkYPVJx/OBucCCGNQUteKcTPxeQ9MhRURO4utD2wBWz/wR4OQVux4HfgJEenmPefYWE36vh1FDAlpjRkTkJNGGux8r2F8CFvfw+lTg9/Z+DnAD1lj9Gye1W2hvADFZN2BMbpA1+47G4q1ERFwjmmEZA3gWa6z9sdO0KcYasikCFgHf59RgPy9KQkEq6lppbOvsjx8nIpIQogn3WcBdwFeAdfZ2A/CgvTmq1D6pqqEZEZEvRTMssxyr9x6te86ulLNTEjo2Y6aJKSMG9eePFhGJWwl9hSpAwcB0Aqk+tuuuTCIixyV8uBuGQUkowHYNy4iIHJfw4Q5f3pXJNHXjDhERcEu4h4IcbenkcGO706WIiMQFV4R7ybFlCDQ0IyICuCTcx4S0xoyISHeuCPfBgVRyAqkKdxERmyvCHayLmTQsIyJicU24j8kNsqOqkUhEM2ZERNwT7qEgbZ0R9te2OF2KiIjj3BPu9owZre0uIuKicB8dCmAYWkBMRARcFO4ZKT6GD8rQjBkREVwU7mCtEKkZMyIiLgv30twge2uaaQ93OV2KiIijXBXuY3KDdEVMdlc3O12KiIij3BXux5YhqNLa7iKS3FwV7kU5maR4PWyvbHK6FBERR7kq3P1eDyOHZOquTCKS9KIJ90LgI2ALsBl4uIc2dwIbgI3ACmBCrArsq1L7xh0iIsksmnAPAz8GxgKXAvPt/e72AlcAFwO/ABbGsMY+KckNcrC+jYa2TqdKEBFxXDThfghYa+83AluB/JParACO2vurgIKYVHcWSu1lCHao9y4iSayvY+5FwCRg9Rna3Af8+WwLOldjcrMA3ZVJRJKbrw9tA8BrwCPA6c5YXokV7rNP8/o8eztvhg1II5jq07i7iCS1aMPdjxXsLwGLT9NmPPAMcD1w5DRtFvLlePx5WXjdMAxKdFJVRJJcNMMyBvAs1lj7Y6dpMxwr9O8CdsSmtLN3bI0Z09SNO0QkOUUT7rOwQvsrwDp7uwF40N4A/g4YDDxhv74m5pX2QWlukLqWTg43tjtZhoiIY6IZllmO1Xs/k/vtLS6UhL68ccfQrDSHqxER6X+uukL1mGPTITXuLiLJypXhPjAzhaHBVE2HFJGk5cpwB2v5X/XcRSRZuTfcQ0F2VjfSFdGMGRFJPq4N95LcIG2dEfbXtjhdiohIv3NtuOukqogkM9eG++ihQQxD4S4iycm14Z6e4mXEoAx2aMaMiCQh14Y7WDNmtumuTCKShNwd7qEg+4600NbZ5XQpIiL9yt3hnptFV8Rk92HdMFtEkovLwz0A6KSqiCQfV4d70eBMUrweLUMgIknH1eHu83oYNTSgnruIJB1XhztYFzPpZtkikmxcH+4loSAH69uob+10uhQRkX7j+nA/tgyBLmYSkWTi+nAfozVmRCQJuT7c8wakEUzzKdxFJKm4PtwNw2BMKKjpkCKSVKIJ90LgI2ALsBl4uIc2BvAbYBewAZgcqwJjocS+K5Np6sYdIpIcogn3MPBjYCxwKTDf3u/uemC0vc0DFsSwxnNWmhukvrWT6sZ2p0sREekX0YT7IWCtvd8IbAXyT2rzdeB3gAmsArKBvBjVeM5KQtZJ1W0adxeRJNHXMfciYBKw+qTj+cCBbs/LOfUXAFi9+jX21m+OT4dUuItIkvD1oW0AeA14BDjbRdIX2htYvfx+kZ2RQigrVT13EUka0fbc/VjB/hKwuIfXK7BOvB5TYB+LGyWhINurdOMOEUkO0YS7ATyLNdb+2GnavAXcbbe9FKjHGquPG6W5QXZWNdHUHna6FBGR8y6acJ8F3AV8BVhnbzcAD9obwJ+APVhTIZ8Gvh/zSs/RjeOH0R6O8PtP9ztdiojIeRfNmPtyrB75mZhYUyTj1sTCbGaMHMzTy/Zw14wRpPq8TpckInLeuP4K1e4emjOKqoZ23vg8rk4HiIjEXFKF+2WjcxiXn8VTS/bQFdHVqiLiXkkV7oZh8NAVF7Cnppn3Nlc6XY6IyHmTVOEOcN24XIpzMnni491aa0ZEXCvpwt3rMXjg8pFsrKjnk11HnC5HROS8SLpwB5g7OZ9QVioLluxyuhQRkfMiKcM91efl/tkj+WTXEdYfqHO6HBGRmEvKcAf4zvThZKX5WPDxbqdLERGJuaQN90Cqj+/OLOLdLZXsqm5yuhwRkZhK2nAHuGdmEak+D08tUe9dRNwlqcN9cCCVb08bzhvrKjhY1+p0OSIiMZPU4Q5w/2XFmCY8s2yv06WIiMRM0od7wcAMvjZxGK98up+jzR1OlyMiEhNJH+4AD14xitbOLl5cuc/pUkREYkLhjnWXpq9eGOKFFfto1s08RMQFFO627185irqWTn7/2YHeG4uIxDmFu23y8IFMLx7EM8v20BGOOF2OiMg5Ubh389CcURyqb+ONdbqZh4gkNoV7N1eUDGFsXhZPLtlNRDfzEJEEFk24PwdUA5tO8/oA4D+B9cBm4N7YlNb/DMPgoTmj2HO4mfe2VDldjojIWYsm3F8ArjvD6/OBLcAEYA7wKyDlXAtzyvXjchkxOIMFH+/SzTxEJGFFE+5LgdozvG4CQcAAAnbbhJ1P6PN6eODyUawvr2flbt3MQ0QSUyzG3P8VuBA4CGwEHgYSerrJNybnMySYygItKCYiCSoW4X4tsA4YBkzECvus07SdB6yxt7iV5vdy3+xilu2sYWN5vdPliIj0WSzC/V5gMdbwzC5gL1B6mrYLgan2FtfunD6cYJpPt+ITkYQUi3DfD1xl74eAMcCeGLyvo4Jpfu6eMYI/b6pk92HdzENEEks04f4KsBIrtMuB+4AH7Q3gF8BMrPH2D4GfADUxr9QB984qJsXr4Undik9EEowvijbf6eX1g8A1Magl7uQEUrlz+gieX7GXO6YPZ9LwgU6XJCISFV2h2osfXT2aUDCN/7F4o9acEZGEoXDvRTDNzy9uGce2ykYWLtXwjIgkBoV7FK4eG+LG8Xn85sNd7KrWyVURiX8K9yg9evNY0vwefrZ4oxYVE5G4p3CP0tBgGv/zxrF8uq+WVz7b73Q5IiJnpHDvg9umFjBz1GD++U/bqGpoc7ocEZHTUrj3gWEY/OPci+noivB3b55uBWQREecp3PuoKCeTH11dwrubq3hn0yGnyxER6ZHC/SzcP7uYi4Zl8bdvbqa+tdPpckRETqFwPws+r4dffnM8tc0d/POftzpdjojIKRTuZ2lc/gDun13MK58e0E09RCTuKNzPwSNfLWH4oAx+9vpG2jq7nC5HROQ4hfs5SE/x8o9zL2ZvTTO/+XCn0+WIiByncD9Hs0fncOuUAp5auoctBxucLkdEBFC4x8Tf3HAhAzP8/HTxBsJdWjlSRJyncI+BgZkpPHrzRWwor+eFFfucLkdEROEeKzeNz+Oq0qH86r0dHKhtcbocEUlyCvcYMQyDX9wyDo8BP3t9I6aplSNFxDkK9xgalp3OT64vZdnOGl7/vMLpckQkiUUT7s8B1cCZVsqaA6wDNgNLzr2sxPXfpo9g8vBs/uHtLdQ0tTtdjogkqWjC/QXgujO8ng08AXwNuAi47dzLSlwej8Evvzme5vYwP/rDOl3cJCKOiCbclwK1Z3j9DmAxcOwOFtXnWlSiGx0K8r9uGcfyXTV874XPaG4PO12SiCSZWIy5lwADgY+BMuDuGLxnwvvWtOE8dvsEVu05wt3PfarVI0WkX8Ui3H3AFOBG4Frgb7ECvyfzgDX25npzJxXw2zsms6G8jjufWUVtc4fTJYlIkohFuJcD7wLNQA3WMM6E07RdCEy1t6Rw/cV5LLxrKjurmvj2wpVU6/Z8ItIPYhHubwKzsXrwGcB0QIucd3Nl6VCev2ca5Udbuf2plVTUtTpdkoi4XDTh/gqwEhiD1Uu/D3jQ3sAK8neADcCnwDOcedpkUpp5QQ7/dt8lHGnu4PYnV7KvptnpkkTExQynrqScOnWquWZNUgy9n2BTRT13Pbsav9fDS/dPZ3Qo6HRJIpJADMMoM02z16FtXaHaz8blD+APD8zABL61cBWbKuqdLklEXEjh7oCSUJBXH5hBut/Ld55eRdkXR50uSURcRuHukKKcTP7wwKUMzkzhrmdX6z6sIhJTCncHFQzM4D8emEF+djr3PP8pH21P+ot7RSRGFO4OG5qVxh8emMEFQwPM+90a3tl0yOmSRMQFFO5xYFBmCi//xaVcnD+A+S9/zkurv9B68CJyThTucWJAup9/u286sy7I4W9e38SPX11Pa4dWlBSRs6NwjyOZqT6ev2caj3x1NK9/XsEtv/2EPYebnC5LRBKQwj3OeD0Gj3y1hBfvvYTqxja+9q+f8McNGocXkb5RuMepy0uG8McfXsboUID5L6/l7/9zMx3hiNNliUiCULjHsWHZ6fxh3gzunVXE85/s41sLV3JQi46JSBQU7nEuxefh0Zsv4rd3TGZHZSM3/b/lLN1x2OmyRCTOKdwTxI3j83jrB7MZEkjlu89/yuMf7KAroumSItIzhXsCGTUkwBvzZzF3Uj6Pf7CTe57/VHd3EpEeKdwTTHqKl1/dNoF/+sbFrN5by42/Wcba/Vp4TEROpHBPQIZh8J1LhrP4oZn4vAa3P7mS55bv1VWtInKcwj2BjcsfwNt/eRlzxgzlH97ewn0vrtE9WkUEULgnvAEZfp6+ewqP3jyWT3bVcPWvl/LW+oNOlyUiDlO4u4BhGNw7q5g/PXwZxTmZ/PCVz5n/8lqdbBVJYgp3Fxk1JMCiB2fwV9eO4b3NlVzz66V8uLXK6bJExAHRhPtzQDWwqZd204AwcOu5FiVnz+f1MP/KC3hz/mxyAinc9+Ia/nrRehrbOp0uTUT6UTTh/gJwXS9tvMAvgffOtSCJjbHDsnjzL2cx/8pRLCor57rHl7FiV43TZYlIP4km3JcCtb20+QHwGlYPX+JEqs/LX11byqKHZpLq83DHM6v5+VubtU68SBKIxZh7PjAXWBBF23nAGnuTfjJ5+ED++MPLuGdmES+s2McNv1lG2Re68EnEzWIR7o8DPwGiWY92ITDV3qQfpad4+fnXLuLlv5hORzjCbU+u4JfvbKM9rF68iBv5YvAeU4Hf2/s5wA1YJ1bfiMF7S4zNHJXDO49cxi/e3sKCj3fz/pYqrh4bYmJhNhMLswllpTldoojEQCzCvbjb/gvA2yjY41owzc//uXUC116Uy798uJOnl+4hbK8wmZuVxoTCAUwsHMiEwgGML8gmkBqLr4mI9Kdo/tW+AszB6pWXA48Cfvu1J89PWdIfrrowxFUXhmjr7GLzwQbWH6hjfXkd6w7U8e5ma368YcDooQEmFGQzwe7dj8kN4vfqEgmReGY4tdjU1KlTzTVrdF41Xh1t7mB9eR3rD9Sz7sBR1pfXH7/iNdXnYWJhNjdPGMZN4/PIzkhxuFqR5GEYRplpmr2et1S4S1RM06T8aCvrDlg9+yU7DrOruokUr4crS4cwd1I+V5YOJdXndbpUEVdTuMt5ZZommw82sHhtBW+tP0hNUzsD0v3cOD6PuZPymTpiIIZhOF2miOso3KXfhLsiLN9VwxufV/Du5ipaO7soHJTO3In53DIpn5FDAk6XKOIaCndxRFN7mHc3VfLGugo+2VVDxIQJhdnMnTiMmycMY3Ag1ekSRRKawl0cV9XQxpvrKnj984NsPdSAz2Nw6cjBTC8exLTiQUwszCbNrzF6kb5QuEtc2VbZwOtrK1iy4zDbKhsBSPF6GF8wgGnFg7ikeBBTRgwkK83fyzuJJDeFu8StupYO1uw7ymf7alm9t5ZNFfWEIyYeA0pzs7jEDvtpRYMYEtQwjkh3CndJGC0dYdbtr2P13lo+21fL2v1Haeu0lioamZPJtKJBXDculzljhmgGjiQ9hbskrI5whE0H6/lsby2f2oHf0BamNDfIg1eM4qbxefh0hawkKYW7uEZnV4S31h3kySW72VndRMHAdB64fCS3TS3UCVlJOgp3cZ1IxOSDrVU88fFu1h2oIyeQwr2zirlrxgidiJWkoXAX1zJNk1V7almwZDdLdxwmmOrjzktH8L3ZRQwNaslicTeFuySFTRX1LFiymz9vPITP6+G2KQU8cPkohg/OcLo0kfNC4S5JZW9NMwuX7ua1sgrCkQg3jR/GQ3NGcWFe1gntTNPENCFimpjYjyYnHDNNE49h4PXYm2Hg8WiWjsQHhbskpaqGNp5dvpeXVn1Bc0cXPo9xQoifLcMA70mB7/UaJxzLSPFyzUW53DqlgFFaT0fOE4W7JLX6lk5eLTvAkeYOPAZ4DAMDMAwDw37uMb58bmAcbwfWL4Mu06Sry36M2FtPx+ytsqHt+Ho6k4dnc+uUQm6akKeTvRJTCncRB1Q3tPH65xW8WlbOruomUn0errV787MuyMGr4R05Rwp3EQeZpsmG8noWlZXz5roKGtrC5A1I4xuT8/nm5AItgyxnTeEuEifaOrv4YGsVi8rKWbrjMBETpowYyK1TCrhxvIZtpG9iGe7PATcB1cC4Hl6/E/gJYACNwEPA+t7eVOEuyajKHrZZ1G3Y5tKRg/EY0Nll0tEVofPYFjbp7Ip0O2bSGf7y+eBAKleVDuXqsSFmXZCjq3WTRCzD/XKgCfgdPYf7TGArcBS4Hvg5ML23N1W4SzIzTZP15fUsKjtA2Rd1+L0Gfq/n+GOK12M991nHjj/3evD7DPweD3uPNLNk+2Ga2sOk+71cXpLD1WNzuap0KAMzddNyt4o23H1RvNdSoOgMr6/otr8KKIjiPUWSmmEYTCzMZmJh9jm9T3u4i1V7anl/SyUfbKnm3c1VeAyYVjSIq8eGuGZsri7oSlLRjrkXAW/Tc8+9u/8OlAL39/aG6rmLxJZpmmysqOf9LVW8v6Xq+E1RxoSCXD02xNVjQ1ycP+C0F2R1dkVo6eiitaOLlo4wLR1d9hamrTPCuPwsCgbqF4XTYn1CtYjew/1K4AlgNnDkNG3m2RtTp06donAXOX8O1Lbw3pYq3t9SyWf7jtIVMQllpTJicCatHV00d4TtILcCvaMr0ut7zhw1mFunFHDduFwyUqL5wz/+HcvARLlXQH+H+3jgdawx9x3RvKF67iL9p66lg//aVs2HW6upaWonI8VLRqqPDL+XjBQv6Sk+MlO8pKd4yUjxWa/b++kpXnwegyU7DrOorJz9tS1kpni5cXwet04pZFrRwIQIRtM0OdzYzs7qJnZUNbKzuomd9mO638tdM0Zw5yUjGJAR37OX+jPchwP/BdzNiePvZ6RwF0k8pmny2b6jvLrmAH/ceIiWji5GDM7g1skFfGNKAfnZ6Wf93kea2tle2ci2ykb217aQkeIlK91PVpqfrHSf/egnK81HVrqfYJqPVN+pM4RM06SqoZ2d1Y3srGrq9thEfWvn8XYD0v2UhAKMDgXZf6SF5btqSPd7uX1qAd+bXcyIwZln/f9yPsUy3F8B5gA5QBXwKHDsV9uTwDPAN4Ev7GNhoNcfrHAXSWzN7WHe2VTJorJyVu45gmF0G7a5KI/0lJ6nZrZ1drGruomthxrYXtnI9ior0A83th9vk5nipT0cIRw5cz6l+T0nhH7EhN2Hm2hsCx9vk53hp2RokNGhAKOHBigJBbkgFGBIIPWEvzi2HmrgmWV7eWt9BeGIyTVjQ9x/2Uimjoivv0x0EZOI9JsDtS0sXlvBorUHOFDbSiDVx40X5/G1icNoag/bPfIGtlU2sq+mmWOZnerzUBIKMiY3SGnusccshgRTMU2T1s4uGlrDNLR10tDaaT92fx4+4XhXxGTU0EwrwO0gH5yZ0qdwrm5o43crv+DfV39BXUsnEwqzuX92MdePy42L2zsq3EWk30UiJp/tq+XVsnL+ZA/bgLWq5vBBGYwJBSnNyzoe5EWDM+N2vZ2WjjCvra3gueV72VvTTH52OvfOKuL2aYWOXlWscBcRRzW3h1m5+wg5wVRKQoGEnV0TiZh8uK2aZ5btYfXeWgKpPr49rZB7ZhUdnxra2RWhrqWT+tYO6lo6ra21k7qWDupbredHu+1/Y3I+984qPqt6YnkRk4hIn2Wm+vjq2JDTZZwzj8c4fp3AhvI6nl2+l+dX7OP5FfvIzUqjvrWTpvbw6f97A7IzUshO9zMgw09OIKVfev7quYuI9NHBulb+fdUXVDa0MdAO7uwMPwO67WenpzAgw08w1RfTO3mp5y4icp4My07nr68rdbqMM3L+1K+IiMScwl1ExIUU7iIiLqRwFxFxIYW7iIgLKdxFRFxI4S4i4kIKdxERF3LsClXDMA7z5TLBfRIKhXKqqqpqYlxSwtPncip9JqfSZ3KqBPtMRpimOaS3Ro6F+zlaQxRrxichfS6n0mdyKn0mp3LdZ6JhGRERF1K4i4i4UKKG+0KnC4hT+lxOpc/kVPpMTuW6zyRRx9xFROQMErXnLiIiZ5CI4X4dsB3YBfzU4VrixT5gI7AO66x/snoOqAY2dTs2CHgf2Gk/DnSgLif19Jn8HKjA+r6sA27o/7IcUwh8BGwBNgMP28dd9z1JtHD3Ar8FrgfGAt+xHwWuBCbisulcffQC1i//7n4KfAiMth+TrUPwAqd+JgC/xvq+TAT+1J8FOSwM/BgrNy4F5tv7rvueJFq4X4LVY98DdAC/B77uaEUST5YCtScd+zrwor3/InBLv1bkvJ4+k2R2CFhr7zcCW4F8XPg9SbRwzwcOdHtebh9LdibwAVAGzHO4lngTwvoHDVBpPxf4AbABa9gm4YcgzlIRMAlYjQu/J4kW7tKz2Vh/Xl+P9Wfm5c6WE7dMe0t2C4CRWN+ZQ8CvnC3HEQHgNeARoOGk11zxPUm0cK/AOiFyTIF9LNkd+wyqgdexhq/EUgXk2ft5WJ9RsqsCuoAI8DTJ933xYwX7S8Bi+5jrvieJFu6fYZ3wKAZSgG8DbzlakfMygWC3/Ws4cWZEsnsL+K69/13gTQdriRd53fbnklzfFwN4Fmus/bFux133PUnEi5huAB7HmjnzHPC/nS3HcSOxeusAPuBlkvczeQWYA+Rg9cQeBd4A/gMYjrUK6e0k1wnGnj6TOVhDMibWNNoH+HK82e1mA8uwpg5H7GM/wxp3d9X3JBHDXUREepFowzIiIhIFhbuIiAsp3EVEXEjhLiLiQgp3EREXUriLiLiQwl1ExIUU7iIiLvT/AQVWPPUQWp5dAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the Results\n",
    "======================\n",
    "\n",
    "To see how well the network performs on different categories, we will\n",
    "create a confusion matrix, indicating for every actual language (rows)\n",
    "which language the network guesses (columns). To calculate the confusion\n",
    "matrix a bunch of samples are run through the network with\n",
    "``evaluate()``, which is the same as ``train()`` minus the backprop.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtcAAAJzCAYAAADEAz+lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzs3XmYHFW5+PFvzySBkIQABnPZQdlEBISAIIuguOOCoiDuC8tVEa8/FXdR73XfFUVAkXtF8aq4L6AgggiyBsKqyCKLBMKShZBlZur3x1t9pzPMTCoz3edMqr+f56lnuqur65xKz6RPv/2e9zSKokCSJEnS+PXk7oAkSZJUFw6uJUmSpDZxcC1JkiS1iYNrSZIkqU0cXEuSJElt4uBakiRJahMH15IkSVKbOLiWJEmS2sTBtaQq1svdAUmS1gYOriWN5unADcBN5f1dgW/k644kSRObg2tJo/kS8FzggfL+NcAB+bojSdLE5uBa0urcOeR+f5ZeSJK0FpiUuwOSJrQ7idSQApgMHA/cmLVHkiRNYI2iKHL3QdLENQv4CnAw0ADOJQbYD4z2JEmSupWDa0mSJKlNzLmWNJrPAusTKSHnAfcDr8naI0mSJjAH15JG8xxgEXAIcDuwLfCenB2SJGkic3AtaTTNSc8vBH4ELMzYF0mSJjyrhUgaza+IBWQeBf4d2BhYlrVHkiRNYE5olLQ6GxER635gGjADuDdrjzpjPeD/AVsCRwHbATsQHzAkSarEtBBJo1kPeCvwzfL+psCcfN3pqNOB5cA+5f27gf/M1x1J0trIwbWk0ZwOrCAWkoF6DzifSFRHWVneX0rU9pYkqTIH15JG000DzhXAVGI1SohrX56vO5KktZETGiWNppsGnB8FfgdsAZwJ7Au8IWeHJElrHyc0ShrNs4EPATsRS583B5wX5OtSRz0O2JuIzl8KLMjbHUnS2sbBdXttBmzFqt8IXJipL1K7dMuAc19gLvAIsQrl7sBXgDtydkodsR9RDeZ0orzkdOC2rD3qrF5gNqu+N/0zU1+k2nNw3T6fAQ4HbiBKlkF8lf7ibD2Sxu+AEfbX8UPjtcCuwC7EoOvbwCuBZ+TslNruo0TFmx2A7YkKOD8iPlzV0XHENc8HBsp9BfF7XlcbE+U0t2bVDxRvytIbdR1zrtvnpcR/1nXNR1V3al3qfF1gL+BK4Jl5utNRfcSg4yXAScTg+s1Ze6ROOBR4KnBVef8eonZ7XR1PvDc9kLsjCf0cuAj4A4PBLikZB9ftcyswGQfXqpcXDbm/BfDlHB1JYDHwfuC1wP5ENaXJWXukTlhBfIhqfm07LWNfUriTWASqm6wHnJC7E+peDq7bZymRr3keqw6w35GnO0msA7ycx3719vEsvVEKdwFPyt2JDjkcOJL46vheYqXGz2XtkTrhf4FvARsQqQNvAk7N2qPOeFf581ZiAvKvWfW96YupO5TQr4AXAL/J3RF1JwfX7fOLcusmPyciIldixL6uvsZghK8H2I3Br9Pr5l7gJ8REN4iJmz/N1x11yOeJKjiLiHSJjwC/z9qjzmimuvyz3KaUW50tJv6/agAfIN6XVpb3C2D9fF1TN3FCo8bjOmDn3J1QR72+5XYfcDtwcZ6udNxRwNHARkQ97+2Ak4Fn5eyU2m4asIzIxd2h3H7L4EJJddZDVEZZlLsjUp25QmP7bAf8mKgWcmvLVmd/AZ6SuxPqqDNatjOp78Aa4G1ExYjmwOPvwOPzdUcdciGR0rYZsWjQa4Hv5uxQh32fiNhOIwIiN7DqROU62pfBXPrXECkwW+brjrqNg+v2OR34JhHdOwj4b+B7WXvUOfOIsmX7ESkCN5f3m/tVH83XtHW7CPgSUf+6TpYTk92aJjGYEqP6aBBzZF5G/J/9CuDJWXvUWTsRHxhfSkTotyE+UNTZN4nXeFfg/wH/AP4na4/UVcy5bp+pxGTGBrHoxIlELvJHMvapUw7J3QEl81vi6/Pvl/ePIGbi30tE+4ZWE1mb/YnI05xK5OS+Ffhl1h6pExrAPsCrGSy12JuvOx03udxeCnydSH+p+4fG1rKaX8eymkrMwXX7LCe+Cfg78HbgbiK3rY6aK9Y9kagesRw4kFiU4L8z9SmVblvp7GBipcKmecS3FbsTX7fWyfuIN+B5wDFEpYHTsvZInfBOouTiT4HrgScAf8zao876FjFX4hoiJWYr6p9z3Syr+RpiISzLaiopJzS2z57AjUR5p08AM4HPEstF19VcYqWzrYmByM+Jr1dfkLFPndSNK51dQ0z0u6y8vycx4NwVuJpYjENrt24tqbkekTrQjSYR0d26+jeirOblRBrblkQAqO7BH00QDq41Hs0I5nuBR4mybXUecN0CPI3uWulsT+A7xLcwDSLi9RYi4vdComZwXexLpHNtRQw+muW7npCxTyn8jsGSmq2r2X0hT3c6bh8iTWA6Mejalfim4q05O9UBryHm/bxrhMfrXOdaysq0kPaZA3yQwTfmpjpHNVcCrwJex2DubZ2/euvGlc4uJyrCzCzvt15/nQbWEAOu/+Cxg8y62xx4Xu5OJPRl4LkMrktwDZE6UDfNahnDLe1e16jan4mJ9s16103WuVZSDq7b50yivNE8BlMG6u6NwLHAfwG3EbPQ6zgju5tXOhtpQm4dUwYWEhM4u02zpOa83B1J6M4h9+v4Yepb5c8/8NgSmvsm7ksq+5U/h/tAISXj4Lp97qf7Vmi8gVWXd78N+EymvnRSN6501vRIy+11iUoxN2bqS6f9kVju/GxW/fBU1xUp5xHRvEnEB+VbietuRvnq+q3bncDTiWucDBxPfX+nIdL1dq+wr266bfK5JhBzrtvnWUSKxHms+sZ8dp7udNT/Aq9k8M15qLq+KSsmv51DTA6qm+EqRhTAM1N3JJGtVvP4Hat5fG01C/gKUQmnAZxLDLDrNpdiH+JDxDuJuvRN6wOHErnmddWNk881gRi5bp83AjsSkZDWP+Y6Dq6PL392W73r3xMLTjxc3t8QOIvI3+wW6xE5unV0UO4OJNatJTUXEDWu624KMWlzEqumSSwCDsvSo3SOJ5a1r9sHJq0lHFy3z57EH3M3+Ff5s66RrZFszODAGuAh6r88duu3E73Ev8En8nWn415IlJNct2VfHfPLW/2EmJC9LXAKUVLz+9S3pObGRHnJrVn1PfBNWXrTOX8qt0eJsrCtXkGsyVBX3Tj5XBOIg+v2+QuxzOwNuTuSwNCZ2E11n5HdT5TuaubtbUV9Z903tX470Ud8zVrX+rgnE5H5g4ha3ocxWN+7zgaI1/RlRC5us6RmXf2cqH38B+o5kXGoI3js4Pr9wI8y9CWVbpx8rgnEwXX77E0sqnIb9Z8U1K0zsT9IlHr6E/H67g8cnbVHnfefwGuH7PufYfbVwdOJv9drgY8RdZ67oXpIt5XUXA84IXcnEng+8e3DZsBXW/avT30/IDd14+RzTSAOrtunm+rENm00zL7FxJt1Hf2OmGG/d3n/nUT+Zp09ecj9ScAeOTqSwKPlz6XApkS+5ib5upNMt5TUbPoVMej8Te6OdNg9wBXAi4na7U2LiXrudfax3B1Qd7NaSHv0EivW7Zi7I4ndDmxB5B43iKXf7yVSB45i1f/Q66BBTIR6ApGHuyWxzG4dUwfeD3wAmMrgEtENYAWRl/v+TP3qpA8TKRHPAk4ivnk6lZFrfWvttJhYYGU5EQioezrbZOIaty/v30x9AyBNGxMrBw+dP1HXyj+aYBxct8/PifI/3VRH81Tgx0RpNoDnAC8HTidKXT0tU7865ZtEfuozgScR1ULOJSaz1lEPkXtct4leVaxDvCnXeVKUJTW7wzOI6i+3E4PsLYDXAxdm7FOnnQv8EHg38a3M64m1KLohHUgTgIPr9rkQeCoRxWxdeOPFebqTxDxiZbdW1xJvynOB3ZL3qLOuItJCriZea4ilk+tcL3a417iu1gXeSqzyVhD59d8EluXsVAdtQlT+Ganedd2qAe0I3MTIi6fUdbGgK4EjiYg1RAT7B9Q3vQvimvdg8P0I4HLqGwjRBGPOdft8OHcHMvgXEQk4q7x/OJES0ks9l4BfSVxb8xPpxtTzOltdRbwhXZ67Iwn8N5Ey8LXy/pFE7vErsvWos7qtpOa7iAnIXxjmsTovFjSZwYE1wN+o94RVGEx7+RdRXvMehp8jJHWEkev22grYjijxtB4xEFuctUedNYtYBWu/8v7FxESShUQ+8i2Z+tUpryY+QOwOnEGUavsQ9S5pdRNR//gO4huZOlfBuYEop7m6fXUxtKRm87Wtew5yt/kOEQT4Xnn/1cR7U53TvQ4hyi1uQXxYXp94b/pFzk6pezi4bp+jiKjIRsSKZ9sRdXOflbNTarsdide0QSx1f2Pe7nRct6QMQAw+vg5cWt5/GvA2okSd6uXpPHYRmbquSrkO8XvcDIJcBHyDVes/S2ojB9ftMxfYC/grg/m4dc9X3Z6YMLI1q75J1fXr1W8TUZC5LftOLLc625Wo6Q3xxnxNxr500o3EKqvNSclbEl+n91HfaH1T62t8IZGrWlf/QwRA5jK4iEwBvCNbjzprGjFvoHmtvcSAe+mIz1j7PYGYVL8PEbW/hCg/eGvOTql7mHPdPsuJMmVNk6j/6n0/IqLzp9EdK509l1gm+gsMRrleTL0H18cT38qcXd7/HlGK72sjPmPt1Y216uGxr/GZ1Pc1hvgb3on6///cdB5wMLCkvD+VqKbx9Gw96rzvE+U0Dy3vH0FM4qxbBStNUEau2+ezwMPEV8jHEVUHbiBW9aur5ozsbnEVsTT294jo5vHERL+njvaktdy1RPSnWQFnGhEFqnMU9/GsWhu37uU1u+01/hERpf7X6g6sieEqN9WxmlOr1iohTXWv7KQJpCd3B2rkfUQdzXlE7vWvqffAGuCXxIeITYhc8+ZWVw1isuaLiNf6j8DMrD3qvAarfivRX+6roxcDfydWKfwTURe4G5Y/75bX+JfEhLZZRODjnPJ+c6urR1i1/OAcBlcjravfEu/JWxPzRt5LrMhZ9/coTRCmhYzfS4DNia+gTiW+Xt2YiOg+TCyyUlevL3++p2VfQeS71cm2xEqMrW/AJxKDkAMz9Cel04l5BD8t77+UyD2vo08QS9v/gfg24iDgNVl7lEa3vMa/AGYT8wZa7U+9o9jvJKL195T3NyGqHtXZK8ufR5c/mx8Wj6Ce71GaYEwLGb+LiT/YO8v7c4kJfdOJNy2rhaz9fkUs9z1vyP6nAJ8kItl1tjurVhq4OmNfOukKIqp3DTG4HqB7vkruhte42/6O9yTel+4l6lofA7yMiNp/BHgwX9c6pvWaIQJALye+hTqRel6zJiDTQsZvCoMDa4hV3R4k8jSnZelR57235fbQBTY+mbIjiczmsW/IlPu2TtuVZNYlIl5fJ96wvgF8lXoOupoeJj4UX0RM6vsKq662Wjfd9hp329/xtxicZL8P8AHiG9aHiAmrddR6zQcAnyLWJFhIfa9ZE5CD6/HbcMj9t7fc3jhlRxI6ouX2+4c8VseKCxuM8tjUZL1I6wwiijsPeD7w+bzdSeLFRHmy44HfEYsgHZK1R53Vba9xt/0d9zIYqT2cGFz+hFhNeNtcneqwbrxmTUDmXI/fX4k861OH7D8GuCx9d5JojHB7uPt1cAXDv8ZvISqm1NFODNZo/zb1/V2Gx65UCIO/xx8B/kFMTj4vZacS6KbXGLrv77iXeI/vI9ITj255rK7v/d14zZqA/GUbv/8AfgYcSZRqg5jMuA4xMaiOihFuD3e/Dt5JTPZ6NYNvwnOIlKBDR3rSWm5ly+2+bL1IY8Yoj/UCOxNpIjun6U4y3fQaQ/f9Hf+AqHqzgKgO0pzIuS2RJlFH3XjNmoCc0Ng+zwSeXN6+Hjg/Y186rZ/IRW0QX6c2V/pqEHmckzP1q9MOYnCA1S2vMaz6OjeID1DrZ+pXLscQ+Zx10q2vcTf9He9NVAc5l8HXentibsFVIz1pLdeN16wJxsG1JEmS1CZOaJQkSZLaxMF15xy9+kNqpduuF7rvmrvteqH7rtnrrb9uu+Zuu15NAA6uO6fb/qC77Xqh+665264Xuu+avd7667Zr7rbr1QTg4FqSJElqEyc0jqJ32rRi8gYbjem5G603lQeXPjqm564zf9mYnjduPWP/rDVzo2ksfHAci9n194/9ueMwnt//mbNmsHDB4jb2JoFxXe/6LFywaMzPb4zj92tcxnHN68+awaKxvsaNTCXfp4y9WM/MDaex8KGx/R0Xy5ePud3xavT0jul5Mx83nYUPLBl7wwMDY3/uOBTjqHg6nr/jRu/Y/p3bYoz/f8zcaDoLHxz7a1ysTF+lclnxCCuKZXVcMwKA5x40rXjgwTTv+Vdeu/ycoiiSL25nnetRTN5gI7Y65l1jfv5oxXNHs82Xrx9zm+PRmDbO1dpnj/2pAw89PL62x6gYz6B+MVHNfC0yrjeKh4iqz2PUs26mf6zxDICWEMUlxyLTQKSx+SbjO8HMsT1t4B+3j6/dceiZPsb/u1YyroKDxbI8HyiKvnH8HT/MmN/5e2bmq87YGOtrDLDp2J/af+99Y3/yGF26/LfJ20zpgQf7ueycLZO01bvJ32claWgI00IkSZKkNjFyLUmSpCQKYIA8KVWpGLmWJEmS2sTItSRJkhIp6C+MXEuSJEmqwMi1JEmSkoic63qXgTZyLUmSJLWJkWtJkiQlY7UQSZIkSZUYuZYkSVISBQX9hTnX7TAb+D5wK3AlcAlwaKK2JUmSpCRSRK4bwM+AM4Ajy31bAS+u+PxJQF8H+iVJkqTErBYyfs8EVgAnt+y7A/ga0At8DrgcuBY4pnz8QOAi4BfADcDWwE3Ad4G/AWcCBwMXA38H9iqftxcRFb8a+AuwQ7n/DcDZwO/K4z/bvsuTJEmSQorI9ZOBq0Z47M3AQmBPYB1isHxu+djuwM7AbcTgelvgFcCbiMH4kcB+RAT8A8BLiQH4/kSk+2Dgk8DLy/PtBjwVWA7cTAzu7xymT0eXGxutN3WNL1aSJEndK8eExpOIQfEKIoK9C3BY+dhMYLvyscuIgXXTbcC88vb1wHlELfJ5xOC7+fwzynMUwOSW559HDOQhouFbMfzg+pRy48GljxYz1vz6JEmSNIwC6DctZNyuJ6LQTW8DngVsTORjH0dElXcDtmEwcv3IkPMsb7k90HJ/gMEPCZ8A/khEvF8ErDvC8/uxUookSZLaLMXg+nxikPvvLfvWK3+eU+5vRpi3B6aNo62ZwN3l7TeM4zySJEnqgAGKJFsuKQbXBZEP/QwiteMyInXjBOA0IkXjKuA64FuML6L8WeBTxIRGI9OSJElKKtUA9F/AESM89oFya3VBuTXdTqR6NL1hhMcuIaLfTR8qf3633JoOGaWvkiRJ6oACXERGkiRJUjWmTkiSJCmZgdwd6DAj15IkSVKbGLmWJElSEgWFda4lSZIkVWPkWpIkSWkU0F/vwLWRa0mSJKldjFxLkiQpiQKrhUiSJEmqyMj1KNa5fwVPOPX25O0uPnCH5G0CTD//pizt5lSsWJGl3Z7p07O0S39/nnaBxjZbZGm3//qbs7Q7aYvNs7TbP32dLO3S6L5YzcCKlVna7Z25fpZ2mZRxyNCX6f+ugZonB2fRoJ9G7k50VPf9byhJkiR1iINrSZIkqU1MC5EkSVISBfXPtjFyLUmSJLWJkWtJkiQl44RGSZIkSZUYuZYkSVISBUauJUmSJFVk5FqSJEnJDBRGriVJkiRVYORakiRJSZhzLUmSJKkyI9eSJElKoqBBf81ju/W+OkmSJCkhI9eSJElKxmohkiRJkioxci1JkqQkrBYiSZIkqTIH15IkSVKbmBYiSZKkRBr0F/WO7db76iRJkqSEjFxLkiQpiQIYqHlst95XJ0mSJCVk5FqSJEnJWIpPkiRJUiVGriVJkpREUVgtRJIkSVJFRq4lSZKUzIA515IkSZKqMHI9mgbQk/7zx7Rzr0veJsCdb90tS7sAW5x2fZZ2e9dZJ0u7jQ1nZmm377Y7srQLMHDL7Vna7Z39+CztDjy8MEu7PUseydJuMTnj28nkKXnaLQayNDuwdGmWdnsmrZ+lXYBi8ZJMDed4jYsMbaZTAP01j+3W++okSZKkhIxcS5IkKRGrhUiSJEmqyMi1JEmSkiiAgZrHdut9dZIkSVJCDq4lSZKkNnFwLUmSpGT6i0aSraLnATcDtwDvG+bxmcAvgWuA64E3ru6EDq4lSZLUjXqBk4DnAzsBryp/tnobcAOwK3Ag8AVg1OL6TmiUJElSEgWNibSIzF5ExPrW8v5ZwEuIwXRTAcwglhacDjwI9I120glzdZIkSVK7zJ49exZwRct29JBDNgPubLl/V7mv1deBJwH3APOA44FRl+40ci1JkqRkBhItIjN//vwFwJxxnua5wFzgmcATgd8DFwGLRnpCjsh1P9HJ5jZc8nhVS8qfmwI/HuW4rYHrxtGOJEmS6uVuYIuW+5uX+1q9ETibSA+5BbgN2HG0k+aIXD8K7Nbmc94DHNbmc0qSJKmNCphIOdeXA9sB2xCD6iOAI4cc80/gWUS0ejawA4M52sOaMFcH3A58DLiKyGlpfirYmAjBXw+cBtwBzBry3K0ZjEw/GbiMiIpfS/yjQcwIPbU8z7nA1PZfgiRJktYSfcDbgXOAG4H/JcaJx5YbwCeApxNj0/OAE4AFo500R+R6KjHwbfoU8MPy9gJgd+CtwLuBtwAfBc4vj3se8ObVnP9Y4CvAmUSplF7ik8Z2RImVo4h/vJcD3xv31UiSJKmSgjWqQZ3Cb8qt1cktt+8BnrMmJ5xoaSFnlz+vBF5W3t4POLS8/TvgodWc/xLgg0TezNnA38v9tzE4qL+SiHYP5+hyY+ZG01fTlCRJkjRoIqWFACwvf/Yz9oH/94EXE4P43xCzO1vPvbrzn0LMLJ2z8MElIxwiSZKksRigJ8mWy0QbXA/nYuCV5e3nABuu5vgnEInmXwV+DuzSua5JkiRJgyZCzvXvGL0c38eAHwCvJVI+7gUWj3L8K8tjV5bHfhJYfxz9lSRJUhsUBfQnqnOdS47Bde8I+7duuX0FsX47wEKigHcfsA+wJ4MpHs2k6NuBncvbny63Vg+2PA7w+TXrsiRJkrR6a8MKjVsS1T16gBVEtQ9JkiStdRoMMKGqhbTd2jC4/jvw1NydkCRJklan3kkvkiRJUkJrQ+RakiRJNVBQ/wmN9b46SZIkKSEj15IkSUqmv+ax3XpfnSRJkpSQkWtJkiQlUdBgoKh3KT4j15IkSVKbGLmWJElSMnXPuXZwPYpixUr67rwrdzeS2ezLl2Vr+w03/D1Lu995yo5Z2i0eeihLuzkVy5dnabcxKc9/c/2LF2dpl57eLM02evO0Cxn/nooiS7ONRqav1FeuyNMu0L9oUba2k8vza6U2cnAtSZKkJApgwDrXkiRJkqowci1JkqREGvRjtRBJkiRJFRi5liRJUhLmXEuSJEmqzMi1JEmSkjHnWpIkSVIlRq4lSZKURFE0zLmWJEmSVI2Da0mSJKlNTAuRJElSMv2mhUiSJEmqwsi1JEmSkiiAAUvxSZIkSarCyLUkSZISaZhzLUmSJKkaI9eSJElKogAGCnOuJUmSJFVg5FqSJEnJ9Nc8tlvvq5MkSZISMnItSZKkJAoa5lxLkiRJqsbItSRJkpIZqHlst95XJ0mSJCVk5FqSJElJFAX0m3MtSZIkqQoH15IkSVKbmBYiSZKkZOpeis/BtQY18n2RcfouT8rS7svm/jNLu2c/ZdMs7RZ9fVnazanvnn/l7kJSjZ48b1qNyfneTor+/jwNZ/p7Gli+PEu7vTPXz9JuTo111knf6PJ6Dzy7gYNrSZIkJRGLyNQ7K7neVydJkiQlZORakiRJyfRT79QXI9eSJElSmxi5liRJUhIF9a8WYuRakiRJahMj15IkSUrEaiGSJEmSKjJyLUmSpGQGrBYiSZIkqQoj15IkSUqiKKDfaiFJ/RtwFvAP4ErgN8D2bTjvicC723AeSZIkaUQTKXLdAH4KnAEcUe7bFZgN/C1XpyRJktQ+VgtJ5yBgJXByy75rgGcDc8vtbuD08rHXAJeV+78F9Jb7nwdcVT73vJZz7QRcANwKvKMTFyBJkqTuNpEG1zsTqSBDfQTYDTgQeBD4OvAk4HBg3/KxfuDVwMbAqcDLiaj3K1rOsyPwXGAv4KPA5BH6cTRwBXDFzFkzxnM9kiRJ6jITKS1kNA3ge8AXiQH424E9gMvLx6cC9wF7AxcCt5X7H2w5x6+B5eV2H5FuctcwbZ1SbixcsLioebUYSZKkZAoatV/+fCINrq8HDhvhsROJgXAzJaRB5Ga/f8hxLxrl/Mtbbvczsa5dkiRJNTCR0kLOB9Yh0jKadgE+DBzMqnnS5xED8ceX9zcCtgIuBQ4AtmnZL0mSpAligEaSLZeJFL0tgEOBLwMnAMuA24H1gM2IyYsAvyDysD8EnEt8QFgJvI0YXB8NnF3uv4+YEClJkiR13EQaXAPcA7yy4rE/LLehflturU4ccn/nNeuWJEmSxquA2udcT6S0EEmSJGmtNtEi15IkSaoxF5GRJEmSVImRa0mSJKVR1L/OtZFrSZIkqU2MXEuSJCmJArLWoE7ByLUkSZLUJkauJUmSlIw515IkSZIqMXI9mgY0JqX/J2pMmZK8TQB6e/O0C7ByZZZmf3bAk7O0e8BV/8zS7oV7zMzSLkDRl+c1piiyNJvj/w4g29/xwNKlWdoFoFHvKNhjZPqdLpY+mqVdyPf3VKxYkaHRPK9vKq7QKEmSJKkyB9eSJElSm5gWIkmSpGRMC5EkSZJUiZFrSZIkJVHg8ueSJEmSKjJyLUmSpGRc/lySJElSJUauJUmSlEZhtRBJkiRJFRm5liRJUhIufy5JkiSpMiPXkiRJSsbItSRJkqRKjFxLkiQpCVdolCRJklSZkWtJkiQlUxi5liRJklSFg2tJkiSpTUwLkSRJUjIDmBYiSZIkqQIj15IkSUqiKFxERpIkSVJFRq4lSZKUjKX4JEmSJFVi5FqSJEmJuPy5JEmSpIqMXEuSJCkZc64lSZIkVWK1FhN3AAAetklEQVTkejQFFANF+maXLk3eZtdatixLs3/aZWqWdh89Z7Ms7QJMff4/8zRc9Odptq8vS7vkajenIv3/0wDFvrtlabdx8dws7Q4sWZKlXSDba9x/4O7pG73iL+nbTKjAOteSJEmSKjJyLUmSpDSKbF9EJGPkWpIkSWoTB9eSJElKZoBGkq2i5wE3A7cA7xvhmAOBucD1wJ9Wd0LTQiRJktSNeoGTgGcDdwGXA78Abmg5ZgPgG8Qg/J/A41d3UiPXkiRJ6kZ7ERHrW4EVwFnAS4YccyRwNjGwBrhvdSd1cC1JkqQkCmIRmRTb7NmzZwFXtGxHD+nOZsCdLffvKve12h7YELgAuBJ43equ0bQQSZIk1c78+fMXAHPGeZpJwB7As4CpwCXApcDfRnuCJEmSlEBjIi0iczewRcv9zct9re4CHgAeKbcLgV0ZZXBtWogkSZK60eXAdsA2wBTgCGJCY6ufA/sRAen1gKcBN452UiPXkiRJSmYCLSLTB7wdOIeoHPIdotzeseXjJxMD6d8B1wIDwGnAdaOdtMrguh+YVx57I/B6YOkadPwDwCfX4HhJkiQphd+UW6uTh9z/XLlVUiUt5FFgN2BnokzJsaMf/n8a5fk/ULUzkiRJqrdU1UJyWdOc64uAbcvb7yLC4tcB7yz3bU2scvPf5f5vEzMr5wJnlo+3htLfDZxY3t6TCLnPJT4dNI97A/D1luf8ilgpB+A5xKzNq4AfAdPL/Z8mCoBfC3y+3Lcx8BMiv+ZyYN/KVy1JkiRVsCY515OA5xN5J3sAbySSuhvAX4nlIB8iEsNfT5QpAXgFEfmGGFyP5HTgKGKw/OkK/ZkFfAg4mJi9eQIx4D8JOBTYkSinuEF5/FeALwF/BrYk8mueVKEdSZIktUFRkDWqnEKVwXUz8gwRuf428O/AT4lBLcTKNfsTMyzvYHBgXdUGwAxiYA3wfeCQ1Txnb2An4OLy/pTy+QuBZWU/f1VuEIPwnVqevz4R6V4y5LxHlxszZ60fHxckSZKkCqoMrps511U9MspjfayairJuhfON9JwG8HvgVcM8Zy+i2PdhxCzQZ5bn2JsYeI/mlHJj4YJFBb0VeihJkqRKJlCd644Ya53ri4CXEvX+phFpGBeNcOxKYHJ5ez7weOBxwDoMRqcfBhYTaSYQdQabbicG9z1Eoe+9yv2XEnnTzRzwacQSldOBmcTMz/8gCn0DnAsc13LeNfnAIEmSJK3WWOtcXwV8F7isvH8acDXD51SfQkwsvAp4NfDx8nl3Aze1HPdm4FSihuCfiPQOiLSP24gJijeW5wG4n5js+ANioA6Rg72YKPi9LhHdflf52DuIfOxrieu+kOqVTyRJktQGE6jOdUdUGVxPH2H/F8ut1e1Eyb5WJ5Rb01fLbajrgV3K2+8DrihvF8SgfDjnE1VGhtprmH0LgMNHOI8kSZI0bhNphcYXAu8n+nQHEZWWJElSjVgtJJ0flpskSZK0VhrrhEZJkiRJQ0ykyLUkSZJqrCDv0uQpGLmWJEmS2sTItSRJkpKpeSU+I9eSJElSuxi5liRJUhpF/UvxGbmWJEmS2sTI9SgaUyYzadNNMzSc5xNd3+3/zNJuN+qZMSNLu9MOnZ+lXYCbvzYnS7vbH39llnZ7NtwwS7uNddfJ0u7AAw9maReg6O/P0/DFc7M02zNtWpZ2G1OmZGkXoFi+PE/DF1yVvs1iafo2U6t50rWRa0mSJKlNjFxLkiQpGXOuJUmSJFVi5FqSJEnJFOZcS5IkSarCyLUkSZKSKDDnWpIkSVJFRq4lSZKURgEYuZYkSZJUhYNrSZIkqU1MC5EkSVIyluKTJEmSVImRa0mSJKVj5FqSJElSFUauJUmSlEjDRWQkSZIkVWPkWpIkSemYcy1JkiSpCiPXkiRJSqPAnGtJkiRJ1Ri5liRJUjrmXEuSJEmqwsi1JEmSEjLnWpIkSVIFRq4lSZKUjjnXkiRJkqpwcC1JkiS1iWkhkiRJSse0EEmSJElVGLmWJElSGgVQ8+XPHVyPpq+fgfsfSN5ssbIveZsANPL9si974Z5Z2p16ztws7Q48sjRLuxQDedoFtjvuiizt3vGRp2Vpd8uP/zVLu5M2mZ2lXbbdMk+7QDHv5jwNZ/o/s7HeelnaHVi0KEu7kPF9URoDB9eSJElKpjDnWpIkSVIVRq4lSZKUjpFrSZIkSVUYuZYkSVI6Na8WYuRakiRJahMj15IkSUqmYc61JEmSpCqMXEuSJCmNAquFSJIkSarGyLUkSZISaVgtZAyWdOCckiRJ0oRnWogkSZLUJp0aXE8HzgOuAuYBLyn3bw3cBJwJ3Aj8GFivfOwjwOXAdcApQPM7gwuAzwCXAX8D9i/39wKfK59zLXBMuX8T4EJgbnmu5vHPAS4p+/Sjso+SJElKqUi0ZdKpwfUy4FBgd+Ag4AsMDpZ3AL4BPAlYBLy13P91YE9gZ2AqcEjL+SYBewHvBD5a7nszsLB8zp7AUcA2wJHAOcBuwK7EIHsW8CHg4LJPVwDvGqHvR5ePX7H+4xx/S5IkqbpOTWhsAJ8EDgAGgM2A2eVjdwIXl7e/B7wD+DwxCH8vEcneCLge+GV53NnlzyuJ6DdEJHoX4LDy/kxgOyKS/R1gMvAzYnD9DGCnlnanEFHs4ZxSbix6YEnB1DW4akmSJI2u5qX4OjW4fjWwMbAHsBK4HVi3fGzoP2lRPvYNYA4x+D6x5XiA5eXPfgb73ACOI6LUQx0AvBD4LvBF4CHg98CrxnQ1kiRJUgWdSguZCdxHDKwPArZqeWxLYJ/y9pHAnxkcSC8gcqEPY/XOAf6diFADbA9MK9uaD5wKnEakgVwK7AtsWx47rTxekiRJKdU857rdketJRJT5TCKlYx6Rv3xTyzE3A28jUjduAL4JLCUGw9cB9xKpHatzGpEichURxb4feClwIPAeYmC/BHhd+dgbgB8A65TP/xAxQVKSJElqi3YPrp8M/IOIQO8zzONbA33Aa4Z57EPlNtSBLbcXMJhzPQB8oNxanVFuQ51PTHyUJElSDgUuIrMGjiUiw8MNkCVJkqTaa2fk+uRyG83tRKk9SZIkdaFGzauFuEKjJEmS1CadKsUnSZIkPZaRa0mSJElVOLiWJEmS2sTBtSRJktQm5lxLkiQpGauFSJIkSarEyPUoioEBBpYuTd7upM02Td4mQN/d92RpF2C986/L0m5RDGRpt9GTZ3WqYiDf5+meKZOztLvlx/+apd2B3+f5O+Z1/VmaHbj2piztAvSsu26WdgeWLcvT7qJFWdrt3WjDLO0C9M2/P0/DPb3p28zzJ5yWKzRKkiRJqsLBtSRJktQmpoVIkiQpjQIXkZEkSZJUjZFrSZIkpWPkWpIkSVIVRq4lSZKUjIvISJIkSarEyLUkSZLSMXItSZIkqQoj15IkSUrHyLUkSZKkKoxcS5IkKYlGYbUQSZIkSRUZuZYkSVI6RSN3DzrKyLUkSZLUJkauJUmSlI4515IkSZKqcHAtSZIktYlpIZIkSUrGUnySJEmSKjFyLUmSpHSMXEuSJEmqwsi1JEmS0nD5c0mSJElVGbmWJElSOkauJUmSJFVh5FqSJEnpGLmWJEmSVIWR61E0envpnblh+oYHBtK3CdBo5Gk3Y9tFf3+Wdilq/rF9WJNzdyCpSS99OEu7D/7v47O0O/NFGd9OJmf63Vq2LEuzPRvMzNJu8cjSLO0C9EzJ8xoPLF+epd26s1qIJEmSpEocXEuSJKlbPQ+4GbgFeN8ox+0J9AGHre6EDq4lSZLUjXqBk4DnAzsBryp/DnfcZ4Bzq5zUwbUkSZLSKRJtq7cXEbG+FVgBnAW8ZJjjjgN+AtxX5aQOriVJklQ7s2fPngVc0bIdPeSQzYA7W+7fVe4besyhwDertmu1EEmSJNXO/PnzFwBzxnmaLwMnAJVLuTm4liRJUhrFhCrFdzewRcv9zct9reYQ6SIAs4AXEBMbfzbSSR1cS5IkqRtdDmwHbEMMqo8AjhxyzDYtt78L/IpRBtbg4FqSJEkpTZzIdR/wduAcoiLId4DrgWPLx08ey0kdXEuSJKlb/abcWo00qH5DlRM6uJYkSVI6Eydy3RETrRRfPzAXuA74EbDeao5fUv7cFPjxKMdtXZ5TkiRJ6piJNrh+FNgN2Jko5n3s6If/n3uosBylJEmS8mkQ1UJSbLlMtMF1q4uAbcvb7yIiz9cB7xzm2K0ZjEw/GbiMiIBfS8wChUhUP5VIVD8XmNqJTkuSJKl7TdTB9SRinfd5wB7AG4GnAXsDRwFPHeW5xwJfISLgc4jVdiAG2ScRg++HgZeP8PyjKVfymfm46eO6CEmSJA0xcZY/74iJNrieSkScrwD+CXwb2A/4KfAIkWN9NrD/KOe4BPgAsZrOVkSqCcBt5bkBriSi3cM5hRiUz1n4wJIRDpEkSZIea6JVC2nmXI/H94G/Ai8kSqscA9wKLG85ph/TQiRJktKaWCs0dsREi1wP5yLgpUTlkGnAoeW+kTyBGEx/Ffg5sEunOyhJkiTBxItcD+cqYrnJy8r7pwFXj3L8K4HXAiuBe4FPAut3sH+SJEmqquaR64k2uB5pBuEXy22k428nyvcBfLrcWj3Y8jjA58fYP0mSJGlEE21wLUmSpDqreeR6bci5liRJktYKDq4lSZKkNjEtRJIkSclYik+SJElSJUauJUmSlI6Ra0mSJElVGLmWJElSGgVGriVJkiRVY+R6FEV/P/0PPZS+4UYjfZtAY9LkLO0C9Gy4QZZ2G1OmZGk3y+9VZgPLlmVpd9IWm2dpt+/uf2Vpd4PD7s3S7hMv6c3SLsDf91ycre0c+u9/IEu7jZ48700ARV9ftrbVflYLkSRJklSJkWtJkiSlY+RakiRJUhVGriVJkpSMOdeSJEmSKjFyLUmSpHSMXEuSJEmqwsi1JEmS0nCFRkmSJElVObiWJEmS2sS0EEmSJCXRKLc6M3ItSZIktYmRa0mSJKXjhEZJkiRJVRi5liRJUjIufy5JkiSpEiPXkiRJSsfItSRJkqQqjFxLkiQpHSPXkiRJkqowci1JkqQ0CquFSJIkSarIyLUkSZLSMXItSZIkqQoj15IkSUrGnGtJkiRJlTi4liRJktrEtBBJkiSlY1qIJEmSpCqMXEuSJCmZuk9odHC9Oj29yZts9KZvE6Bn+rQs7QL0z78/S7uN3jxf3vTMmJGl3YElS7K0C0Ajz791393/ytJurr/jgUeXZWn3ln0aWdoF2PnKPL9b1+0xkKXdnnXXydJuTsVAptHYQH+edrVWc3AtSZKkNArMuZYkSZJUjZFrSZIkpWPkWpIkSVIVRq4lSZKURIP6Vwsxci1JkiS1iZFrSZIkpWPkWpIkSVIVRq4lSZKUTKOod+jayLUkSZLUJqkG1/8GnAX8A7gS+A1wNPCrEY4/DdgpTdckSZKURJFwyyRFWkgD+ClwBnBEuW9X4MWjPOctne6UJEmS1G4pItcHASuBk1v2XQNcBEwHfgzcBJxJDMQBLgDmlLeXAP9VPudSYHa5f2PgJ8Dl5bZvuf8ZwNxyuxqYUe5/T3nctcDH2nRtkiRJ0v9JMbjemUgFGc5TgXcSKSBPYHCA3GoaMajeFbgQOKrc/xXgS8CewMuJVBKAdwNvA3YD9gceBZ4DbAfsVe7fAzhgHNckSZKkMWgUabZcclcLuQy4q7w9F9ga+POQY1YwmJt9JfDs8vbBrJqXvT4RCb8Y+CIRCT+7PP9zyu3q8tjpxGD7wmH6dHS5MXPW+vDQml+UJEmSulOKwfX1wGEjPLa85XY/w/dnJYNp6a3H9AB7A8uGHP9p4NfAC4iB9nOJdJNPAd+q0N9Tyo2FCxYV9FZ4hiRJkqqpdyW+JGkh5wPrUEaDS7sQKRvjcS5wXMv93cqfTwTmAZ8hcqx3BM4B3kRErAE2Ax4/zvYlSZKkVaSIXBfAocCXgROISPPtwM/Ged53ACcRExQnESkexxI53AcBA0TU/LdEhPxJwCXlc5cArwHuG2cfJEmStAZy5kOnkCrn+h7glcPsP7Xl9ttbbh/Ycnt6y+0flxvAAuDwYc553DD7ICZAfmXUXkqSJEnjkHtCoyRJkrpJzSPXLn8uSZIktYmRa0mSJKWRuQZ1CkauJUmSpDYxci1JkqR0jFxLkiRJqsLItSRJkpJoYM61JEmSpIqMXEuSJCmdot6hayPXkiRJUpsYuV6dgf7kTRYZ2gTof2hFlnZzKlZmanjZsizNTtps0yztAvTdfU+2tnPI9XecSzGQr+3r9sjT7iduuzxLux/eZs8s7XajSdtslbzNxl1Tkrep9nJwLUmSpGSc0ChJkiSpEiPXkiRJSqPARWQkSZIkVWPkWpIkSck0Mk6ATsHItSRJktQmRq4lSZKUjjnXkiRJkqowci1JkqRkrHMtSZIkqRIj15IkSUqjAIp6h66NXEuSJEltYuRakiRJyZhzLUmSJKkSI9eSJElKx8i1JEmSpCocXEuSJEltYlqIJEmSkmjghEZJkiRJFRm5liRJUhpF4SIykiRJkqoxci1JkqRkzLmWJEmSVImRa0mSJKVj5FqSJElSFUauJUmSlIw515IkSZIqMXItSZKkNApgoN6hayPXkiRJ6lbPA24GbgHeN8zjrwauBeYBfwF2Xd0JjVxrQujdcMMs7Q4sXpyl3aKvL0u7fXffk6XdrBqNPO3WfAUywYe32TNLu+fcMzdLu8/baq8s7QIUK1dkabfvtjuSt1kUea41qYnz32MvcBLwbOAu4HLgF8ANLcfcBjwDeAh4PnAK8LTRTmrkWpIkSd1oLyJifSuwAjgLeMmQY/5CDKwBLgU2X91JjVxLkiQpmVTVQmbPnj0LuKJl1ynl1rQZcGfL/bsYPSr9ZuC3q2vXwbUkSZJqZ/78+QuAOW063UHE4Hq/1R3o4FqSJEnd6G5gi5b7m5f7htoFOI3IuX5gdSd1cC1JkqR0Js6E78uB7YBtiEH1EcCRQ47ZEjgbeC3wtyondXAtSZKkbtQHvB04h6gc8h3geuDY8vGTgY8AjwO+0fKcUVNNHFxLkiQpmQm2/Plvyq3VyS2331JulVmKT5IkSWoTI9eSJElKo2AiLSLTEUauJUmSpDYxci1JkqQkGkBj4lQL6YhUkesPErMvrwXmspo12YexG/CClvsHAk9vuX8s8LpRnn8i8O41bFOSJElaIyki1/sAhwC7A8uBWcCUNTzHbkTZk+ZszgOBJcR677DqrE5JkiRNVAO5O9BZKSLXmwALiIE15e17gD2JwfE1wGXADGBd4HRgHnA1sdTkFODjwOFE1PsEIlL9H+X9/Vk1Mv0O4AYiSn5WSz92Ai4Abi2PkSRJktoqReT6XKIA99+APwA/BC4pfx5OrI6zPvAocDwxh/QpwI7lc7cvnz+HKPQNMJWIXH++vP+slvbeR6y0sxzYoGX/jsRgfQZwM/BNYOUw/T263Jg5a0aFRS4lSZJUlTnX47cE2IMYsN5PDKqPAf5FDKwBFhEr3uwHfK/cdxNwBzG4XhPXAmcCrynP2fRrYsC9ALgPmD3C808hBvJzFi5YvIZNS5IkqZulqhbST6RkXECkfLytg229EDgAeBExkfIp5f7lLcf0Y6UUSZKktKxz3RY7ANu13N8NuJHIxd6z3DeDGOxeBLy63Lc9sCWRwrG4PKZp6P2mHmAL4I9EbvZMYHo7LkKSJElanRTR2+nA14j85z7gFiJF5PRy/1Qi3/pg4BtELvS88tg3EBHnPxK51HOBTwG/BH4MvAQ4rqWtXiKtZCZRSvGrwMMdvDZJkiRVVkDNc65TDK6vZNWa1E0LgL2H2f/GYfY9yGCUu2mXltsXtdzeb5jnnzjk/s7DHCNJkiSNi3nHkiRJSqZR78B1shUaJUmSpNpzcC1JkiS1iWkhkiRJSqfmExqNXEuSJEltYuRakiRJaRTQGMjdic4yci1JkiS1iZFrSZIkpWPOtSRJkqQqjFyPptGgMXlK8maLlSuSt5lb0deXpd2eGTOytNu/cFGWdhnoz9NuTo1cMYRMSYW5IkI9vXnahWy/1zneHwCev8P+Wdp90/XXZmkX4DtP2TFLu8Xy5Vnarb16B66NXEuSJEntYuRakiRJyTTMuZYkSZJUhZFrSZIkpWPkWpIkSVIVRq4lSZKURkG2YkqpGLmWJEmS2sTItSRJkpJoUFgtRJIkSVI1Dq4lSZKkNjEtRJIkSemYFiJJkiSpCiPXkiRJSsfItSRJkqQqjFxLkiQpDReRkSRJklSVkWtJkiQl4yIykiRJkioxci1JkqR0jFxLkiRJqsLItSRJkhIpjFxLkiRJqsbItSRJktIoMHItSZIkqRoj15IkSUrHFRolSZIkVeHgWpIkSWoT00IkSZKUjMufS5IkSarEyPUoGkCjN/3nj8aUacnbBGj09mZpF6B/ySPZ2u4qPfle40ZPI1vbeeT5t248ebss7RY3/CNLuwCNddfJ0m7R35+l3caUyVna/e6cXbO0C3DrGVtnafcJr70pfaMru+D/SiPXkiRJkqowci1JkqQ0CmDAyLUkSZKkCoxcS5IkKZHCnGtJkiRJ1Ri5liRJUjpGriVJkiRVYeRakiRJ6Ri5liRJklSFkWtJkiSlYZ1rSZIkSVWlHFx/ELgeuBaYCzytzef/y2oeX9Lm9iRJkrRGCigG0myZpEoL2Qc4BNgdWA7MAqa0uY2nt/l8kiRJ0hpJFbneBFhADKwpb98D3A58FpgHXAZsWz7+IuCvwNXAH4DZ5f4Tge8AFwC3Au9oaaMZmd4EuJCIjl8H7N9yzH8B1wCXtpxTkiRJaotUg+tzgS2AvwHfAJ7R8thC4CnA14Evl/v+DOwNPBU4C3hvy/E7As8F9gI+Ckwe0taRwDnAbsCuxCAbYBoxqN6VGHwfNf7LkiRJ0hopijRbJqnSQpYAexBR5IOAHwLvKx/7QcvPL5W3Ny+P2YRIH7mt5Vy/JiLgy4H7iAj0XS2PX05EtycDP2NwcL0C+FV5+0rg2SP09ehyY/1ZM8zUliRJUmUpJzT2E+kcHwXeDry83N/60aJ5+2tEJPspwDHAui3HLG+53c9jPyBcCBwA3A18F3hduX9ly/mHe17TKcAcYM6iBYtHvyJJkiRV1yzFl2LLJNXgegdgu5b7uwF3lLcPb/l5SXl7JjE4Bnj9Gra1FTAfOBU4jZhEKUmSJHVcqrSQ6UQ0egOgD7iFSL04BNiQKM+3HHhVefyJwI+Ah4DzgW3WoK0DgfcQkeolDEauJUmSlFvNlz9PNbi+kpFL5X0OOGHIvp+X21AnDrm/c8vt6eXPM8ptqOktt39cbpIkSVLbuPy5JEmS0jFy3VFbZ25fkiRJapvcg2tJkiR1jbw1qFNIWYpPkiRJqjUj15IkSUqjAAYGcveio4xc6/+3dwetcZRhHMD/IUU8CqYEUSkNlEKPIuJHaL0UPLUeBC8i6Afwcwhi8eDBk4eeeigU8gEEe6ooWqUgbdFgpEihRNvs62HnsBXdvJudvJPt/H4wJDM7M888ZAkPD8++CwBAT3SuAQBox8w1AABQQ+caAIB2dK4BAIAaOtdzlFIy2dsb+jGaWTsx4Nthsj9c7BFZP7M1WOxy/7dB4k72/hok7lDv6fLt7UHiDqk8/nuQuOsbLw4Sd3/3j0HiDun0pVuDxH3nh3vNY/709kD/s+iN4hoAgEZKMjEWAgAAVNC5BgCgjZKU4ktkAACACjrXAAC0Y+YaAACooXMNAEA7vkQGAACooXMNAEAbpSQTq4UAAAAVdK4BAGjHzDUAAFBD5xoAgGaKmWsAAKCGzjUAAI0UM9cAAEAdxTUAAPTEWAgAAG2UJBNjIQAAQAWdawAA2imW4gMAACroXAMA0ERJUsxcAwAANXSuAQBooxQz1wAAQB3FNQAAzZRJabJVOp/kxyQ/J/n4P15fS/JJ9/qtJK8ddEPFNQAAY7Se5NMkF5KcS3K5+znrQpIz3fZ+ks8OuqmZawAA2jk+M9dvZNqRvtPtf5XkYpLvZ865mOTLTBc6+TrJC0leSvLr/91U5xoAgDF6Ocndmf173bFFz3mKzvUcD/Ngd7tc/eUw125ubm7s7Ozs9v1MR+rx4S9dyXyXtJI53z78pSuZ75JWMuf9w1+6kvkuYel8f+/xYRrxN17M9tk+n6baqUGiNvIwD25sl6sbLWJtbW09n+TmzKHPu+1IKa7nKKWcXOLym0le7+tZVsDY8k3Gl/PY8k3Gl7N8n31jy3ls+R57pZTzQz/DjPtJXp3Zf6U7tug5TzEWAgDAGH2T6QcVTyd5LsmlJNf+dc61JO9mumrIm0n+zJx560TnGgCAcXqS5KMkNzJdOeSLJN8l+aB7/UqS60neyvSDj4+SvHfQTRXXR+fIZ3qOmbHlm4wv57Hlm4wvZ/k++8aW89jyZXHXu23WlZnfS5IPF7nhWinVi2wDAABzmLkGAICeKK4BAKAnimsAAOiJ4hoAAHqiuAYAgJ4orgEAoCeKawAA6Mk/q6baAWjQrqQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 10000\n",
    "\n",
    "# Just return an output given a line\n",
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "#ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "#ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn.state_dict(), str(bundle_root / 'common/classify_name_model_parameters'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pick out bright spots off the main axis that show which\n",
    "languages it guesses incorrectly, e.g. Chinese for Korean, and Spanish\n",
    "for Italian. It seems to do very well with Greek, and very poorly with\n",
    "English (perhaps because of overlap with other languages).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (i2h): Linear(in_features=185, out_features=128, bias=True)\n",
       "  (i2o): Linear(in_features=185, out_features=18, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNN = utils.RNN\n",
    "# init model\n",
    "rnn = RNN(utils.n_letters, utils.n_hidden, utils.n_categories)\n",
    "# fill in weights\n",
    "rnn.load_state_dict(torch.load(str(bundle_root / 'common/classify_name_model_parameters')))\n",
    "rnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (i2h): Linear(in_features=185, out_features=128, bias=True)\n",
       "  (i2o): Linear(in_features=185, out_features=18, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('i2h.weight',\n",
       "              tensor([[-0.1978,  0.0617,  0.2890,  ..., -0.1447,  0.0903,  0.0826],\n",
       "                      [ 0.0872, -0.2252, -0.1797,  ..., -0.0367,  0.0043,  0.0576],\n",
       "                      [-0.0411,  0.1632,  0.6428,  ...,  0.0417,  0.0373,  0.0417],\n",
       "                      ...,\n",
       "                      [-0.1427,  0.1695, -0.2029,  ..., -0.0111, -0.0643,  0.1268],\n",
       "                      [ 0.1716, -0.1276,  0.3121,  ...,  0.0225,  0.0513, -0.0054],\n",
       "                      [-0.3750,  0.1401, -0.1442,  ...,  0.0879,  0.0652,  0.1106]])),\n",
       "             ('i2h.bias',\n",
       "              tensor([ 0.0042, -0.0092,  0.0645, -0.0529, -0.0178,  0.0507, -0.0894, -0.0136,\n",
       "                      -0.0425,  0.0082,  0.0348,  0.0724,  0.0032,  0.0014,  0.0320, -0.0037,\n",
       "                       0.0396, -0.0474,  0.0123,  0.0233,  0.0128, -0.0423, -0.0270, -0.0688,\n",
       "                      -0.0519, -0.0226, -0.0774,  0.0618, -0.0131, -0.0039,  0.1085, -0.0043,\n",
       "                      -0.0326, -0.0477, -0.0247,  0.1268,  0.0412, -0.0087, -0.1123,  0.0463,\n",
       "                       0.0226,  0.0483,  0.0051,  0.0169,  0.0527,  0.0085, -0.0176, -0.0252,\n",
       "                      -0.0124, -0.0098, -0.0274,  0.0055, -0.0241, -0.0087,  0.0294, -0.0539,\n",
       "                       0.0253,  0.0283,  0.0088,  0.0764, -0.0225,  0.0028, -0.0119, -0.1248,\n",
       "                       0.0724, -0.0196, -0.0103,  0.0446,  0.0322,  0.0320,  0.0123, -0.0542,\n",
       "                      -0.0217, -0.0068,  0.0111, -0.0923,  0.0106,  0.0381,  0.0683,  0.0480,\n",
       "                       0.0433, -0.0487,  0.0880, -0.0091, -0.0354,  0.0696, -0.0060, -0.0018,\n",
       "                      -0.0082,  0.0702, -0.0637, -0.0514,  0.0154,  0.0630,  0.0005,  0.0055,\n",
       "                       0.0151,  0.0387,  0.1013, -0.0436, -0.0019,  0.0338,  0.1051,  0.0906,\n",
       "                      -0.0036, -0.0247,  0.0376, -0.0505, -0.0581,  0.0216, -0.0270, -0.0005,\n",
       "                       0.0281, -0.0149,  0.0710,  0.0073, -0.0394,  0.0122,  0.0562,  0.0274,\n",
       "                      -0.0149,  0.0284, -0.0139,  0.0061, -0.0357,  0.0944, -0.0101,  0.1306])),\n",
       "             ('i2o.weight',\n",
       "              tensor([[-1.9051,  0.1875, -0.2755,  ..., -0.1270,  0.0049, -0.0541],\n",
       "                      [-1.6518, -0.0268, -0.2303,  ..., -0.0644, -0.1411,  0.0156],\n",
       "                      [ 1.1362,  0.0502,  0.6517,  ...,  0.0395, -0.0817, -0.0213],\n",
       "                      ...,\n",
       "                      [ 0.6860,  2.0533, -0.1949,  ...,  0.2335, -0.1773, -0.1630],\n",
       "                      [-1.0227, -0.3123, -0.3546,  ..., -0.1283,  0.1981,  0.1786],\n",
       "                      [-0.6649, -0.0392, -0.2136,  ...,  0.7220, -0.2393,  0.6019]])),\n",
       "             ('i2o.bias',\n",
       "              tensor([ 0.8310,  0.7917,  0.6092, -0.8977, -0.5813, -0.3186,  0.2633,  0.0854,\n",
       "                      -0.2022, -0.1367,  0.2668, -0.7396,  0.1404,  0.4393, -1.6262,  0.0807,\n",
       "                       0.2442,  0.9128]))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running on User Input\n",
    "---------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Dovesky\n",
      "12\n",
      "(-0.24) Russian\n",
      "2\n",
      "(-1.73) Czech\n",
      "5\n",
      "(-4.28) Polish\n",
      "\n",
      "> Jackson\n",
      "1\n",
      "(-0.95) English\n",
      "10\n",
      "(-1.55) French\n",
      "12\n",
      "(-1.95) Russian\n",
      "\n",
      "> Satoshi\n",
      "4\n",
      "(-1.11) Japanese\n",
      "15\n",
      "(-1.17) Arabic\n",
      "3\n",
      "(-2.31) Portuguese\n",
      "\n",
      "> Nikolas\n",
      "14\n",
      "(-0.05) Greek\n",
      "1\n",
      "(-4.38) English\n",
      "16\n",
      "(-4.47) Dutch\n"
     ]
    }
   ],
   "source": [
    "def predict(input_line, n_predictions=3):\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(lineToTensor(input_line))\n",
    "\n",
    "        # Get top N categories\n",
    "        topv, topi = output.topk(n_predictions, 1, True)\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(n_predictions):\n",
    "            value = topv[0][i].item()\n",
    "            category_index = topi[0][i].item()\n",
    "            print(category_index)\n",
    "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "            predictions.append([value, all_categories[category_index]])\n",
    "\n",
    "predict('Dovesky')\n",
    "predict('Jackson')\n",
    "predict('Satoshi')\n",
    "predict('Nikolas')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
